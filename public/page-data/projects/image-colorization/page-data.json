{"componentChunkName":"component---src-templates-blog-post-js","path":"/projects/image-colorization/","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"templateKey\": \"project\",\n  \"published\": true,\n  \"title\": \"Deep Image Colorization\",\n  \"slug\": \"image-colorization\",\n  \"date\": \"2021-03-20T00:00:00.000Z\",\n  \"featureImage\": \"images/collage.jpg\",\n  \"pinned\": true,\n  \"tags\": [\"Deep Nets\", \"Self-supervised\"],\n  \"excerpt\": \"A summary on my solution to the grayscale image colorization problem using convolutional neural networks.\"\n};\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\nvar PostButton = makeShortcode(\"PostButton\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"introduction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Introduction\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#introduction\",\n    \"aria-label\": \"introduction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Although this was a school project, this was open-ended as opposed to a consistent assignment so I am able to share much of the code here! Alongside this project page, you can also find more details in the report that was submitted for this work. Generally speaking this page serves as a summary of key points but is not granular enough to re-implement.\"), mdx(PostButton, {\n    text: \"Report\",\n    target: \"/reports/image_colorization.pdf\",\n    mdxType: \"PostButton\"\n  }), mdx(\"h2\", {\n    \"id\": \"model-overview\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Model Overview\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#model-overview\",\n    \"aria-label\": \"model overview permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"I opted to implement the model as detailed in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/1712.03400v1.pdf\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"Deep Koalarization: Image Colorization using\\nCNNs and Inception-Resnet-v2\"), \". The main points of the network are as follows:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"CNN encoder for initial feature extraction\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Additional feature extractor using a pre-trained Inception-ResNet-V2\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Fusion layer to combine both sets of features\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Decoder to upsample and estimate the output from the fused features\")), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/6ec03872ea8bd4d6a6d61f29df89b15d/35ffa/model_overview.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"41.66666666666667%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdqiJFf/xAAXEAADAQAAAAAAAAAAAAAAAAAAEiEB/9oACAEBAAEFAsYol//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABgQAAIDAAAAAAAAAAAAAAAAAAAxASEy/9oACAEBAAY/ArYzUn//xAAaEAEAAwADAAAAAAAAAAAAAAABABEhMUFR/9oACAEBAAE/IcC4WeIjqqe5FKXTqf/aAAwDAQACAAMAAAAQ+A//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAdEAEBAAEEAwAAAAAAAAAAAAABEQAhMUFRgZHw/9oACAEBAAE/ELhUCCkrD1MUlA3kTrD+haDYIfec/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Model Overview\",\n    \"title\": \"Model Overview\",\n    \"src\": \"/static/6ec03872ea8bd4d6a6d61f29df89b15d/6c738/model_overview.jpg\",\n    \"srcSet\": [\"/static/6ec03872ea8bd4d6a6d61f29df89b15d/73b64/model_overview.jpg 300w\", \"/static/6ec03872ea8bd4d6a6d61f29df89b15d/3ad8d/model_overview.jpg 600w\", \"/static/6ec03872ea8bd4d6a6d61f29df89b15d/6c738/model_overview.jpg 1200w\", \"/static/6ec03872ea8bd4d6a6d61f29df89b15d/8b34c/model_overview.jpg 1800w\", \"/static/6ec03872ea8bd4d6a6d61f29df89b15d/35ffa/model_overview.jpg 2224w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Model Overview\"), \"\\n  \")), mdx(\"p\", null, \"I enjoy this network as it combines many of the benefits of transfer learning with an additional bare network for fine tuning as the domain transfer will never be perfect with a pre-trained encoder. The paper linked prior goes into much more detail and was a great guide for my implementation, if you are interested I advise you read it.\"), mdx(\"h1\", {\n    \"id\": \"problem-statement\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Problem Statement\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#problem-statement\",\n    \"aria-label\": \"problem statement permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The problem if image colorization is quite an interesting one the simplifies into an elegant self-supervised problem. Given a grayscale image, can we estimate the additional color channels required to gave a full-color image. This sounds difficult at first but with some tweaks to the images, it's quite easy to formulate this as a simple problem.\"), mdx(\"p\", null, \"In the RGB color space, it's hard to imagine how you may split the original image into a set of components, but if you convert the image to be in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/CIELAB_color_space\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"LAB color space\"), \" the problem can easily be self-supervised. In the LAB color space, the L component is the grayscale component and the AB channels encode color information. Now if we convert our dataset images to be in the LAB space, we easily get our input L channel and our ground truth AB channels! Therefore, the dataset requirements are just full-color images. The problem statement simplified as such, we are looking to estimate a function f such that:\"), mdx(\"p\", null, \"$$\\nf(I_L) \\\\rightarrow (I_a, I_b)\\n$$\"), mdx(\"p\", null, \"Where $f$ will be our large convolutional net.\"), mdx(\"h1\", {\n    \"id\": \"dataset\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Dataset\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#dataset\",\n    \"aria-label\": \"dataset permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"I opted to use the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"hhttp://places2.csail.mit.edu/download.html\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"places2 dataset\"), \" as the context of colorizing an image of scenery or typical views from the human perspective are well captured in this dataset. As the dataset is very large and I had limited hardware to train on, I opted to subset the dataset into smaller splits.\"), mdx(\"p\", null, \"I first created folders to store the subsets.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"from skimage import io\\nimport os\\n\\n# Create New Directories\\nfor name in ['train', 'validation','test']:\\n    path = \\\"dataset/\\\" + name + \\\"/\\\"\\n    os.makedirs(path, exist_ok=True)\\n\")), mdx(\"p\", null, \"I then split images from the validation set into specified counts for each split.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# Setting up Breakpoints\\nnum_train = 1000\\nnum_val = 200\\nnum_test = 100\\n\\nbase_dir = './data/validation/'\\nfiles = os.listdir(base_dir)\\nindex = 0\\nfor i,image in enumerate(files):\\n\\n    test = io.imread(base_dir + image)\\n    if test.ndim != 3:\\n        continue\\n\\n    # Pick what folder to place image into\\n    if i < num_train:\\n        os.rename(base_dir + image, \\\"./dataset/train/\\\" + image)\\n    elif i < (num_train + num_val):\\n        os.rename(base_dir + image, \\\"./dataset/validation/\\\" + image)\\n    elif i < (num_train + num_val + num_test):\\n        os.rename(base_dir + image, \\\"./dataset/test/\\\" + image)\\n    else:\\n        break\\n    index += 1\\n\")), mdx(\"p\", null, \"Now that I have folders containing my data, I needed to work on a PyTorch dataset for data loading. There are a few things to consider when creating the dataloader.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The input image is the grayscale channel (L channel in LAB space) so we'll need to retrieve that component after converting the RGB dataset images\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The two other components (AB in LAB) are our ground truth output so we'll also retrieve those as such\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Pre-Trained ResNet requires a different input image size than the base encoder\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"We'll need to resize the dataset images to have each variant\")))), mdx(\"p\", null, \"All things considered, the following code achieves all of these goals and helps us bring all the necessary data into PyTorch.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# Torch dataloader likes having a class instance that represents the data\\n# We preprocess images here\\n# ref: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\\n# ref: https://discuss.pytorch.org/t/how-to-load-images-without-using-imagefolder/59999\\n\\nfrom skimage.transform import resize\\n\\nclass ModelData(torch.utils.data.Dataset):\\n\\n    def __init__(self, base_dir):\\n        self.base_dir = base_dir\\n        self.all_imgs = os.listdir(base_dir)\\n\\n    def __len__(self):\\n        return len(self.all_imgs)\\n\\n    def __getitem__(self, idx):\\n\\n        # Import Image and get LAB Image\\n        img_name = os.path.join(self.base_dir, self.all_imgs[idx])\\n        image = io.imread(img_name)\\n        image  = image / 255.0\\n        lab = rgb2lab(image)\\n\\n        # L Channel\\n        L = np.expand_dims(lab[:,:,0], axis=2)\\n        L /= 50.0\\n        L -= 1.0\\n\\n        # AB Channels\\n        AB = lab[:,:,1:].transpose(2,0,1).astype(np.float32)\\n        AB /= 128.0\\n\\n        # Scale For Inception\\n        L_inc = resize(np.repeat(L,3,axis=2), (299, 299)).transpose(2,0,1).astype(np.float32)\\n\\n        # Scale for Encoder\\n        L_enc = resize(np.repeat(L,3,axis=2), (256, 256)).transpose(2,0,1).astype(np.float32)\\n\\n        # Build Sample Dict.\\n        sample = {\\\"L\\\":L.transpose(2,0,1).astype(np.float32),\\n                 \\\"L_inc\\\":L_inc, \\\"L_enc\\\":L_enc, \\\"AB\\\":AB,\\n                 \\\"RGB\\\": image}\\n\\n        return sample\\n\")), mdx(\"p\", null, \"Then it's as simple retrieving the relevant categories of images and passing them through the PyTorch \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"DataLoader\"), \".\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# PyTorch Loaders for Train and Validation\\n\\n# Get Dataset Objects\\ntrain_images = ModelData(base_dir=\\\"./dataset/train\\\")\\nval_images = ModelData(base_dir=\\\"./dataset/validation\\\")\\ntest_images = ModelData(base_dir=\\\"./dataset/test\\\")\\n\\n# Pass Into Loaders\\ntrain_loader = torch.utils.data.DataLoader(train_images, batch_size=40, num_workers=2)\\nval_loader = torch.utils.data.DataLoader(val_images, batch_size=40, num_workers=2)\\ntest_loader = torch.utils.data.DataLoader(test_images, batch_size=40)\\n\")), mdx(\"p\", null, \"Now we have all of our required dataloaders to move forward with model construction.\"), mdx(\"h1\", {\n    \"id\": \"model-construction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Model Construction\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#model-construction\",\n    \"aria-label\": \"model construction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Here I will discuss how I constructed the entire model as specified in the original paper.\"), mdx(\"h2\", {\n    \"id\": \"base-encoder\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Base Encoder\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#base-encoder\",\n    \"aria-label\": \"base encoder permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Here is where I construct the layers for the base encoder block. These layers will not be pre-trained and will help in refining results more specific to this dataset as opposed to the dataset the pre-trained net is based on. This is a simple CNN network so we can model it as a sequential model wrapped in a module API class.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class ImgEncoder(nn.Module):\\n\\n    def __init__(self):\\n        super(ImgEncoder, self).__init__()\\n\\n        self.layers = nn.Sequential(\\n            # Conv1\\n            nn.Conv2d(3, 64, 3, stride=2, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(64),\\n\\n            # Conv2\\n            nn.Conv2d(64, 128, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(128),\\n\\n            # Conv3\\n            nn.Conv2d(128, 128, 3, stride=2, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(128),\\n\\n            # Conv4\\n            nn.Conv2d(128, 256, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(256),\\n\\n            # Conv5\\n            nn.Conv2d(256, 256, 3, stride=2, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(256),\\n\\n            # Conv6\\n            nn.Conv2d(256, 512, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(512),\\n\\n            # Conv7\\n            nn.Conv2d(512, 512, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(512),\\n\\n            # Conv8\\n            nn.Conv2d(512, 256, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(256),\\n        )\\n\\n    def forward(self, x):\\n        return self.layers(x)\\n\")), mdx(\"h2\", {\n    \"id\": \"pre-trained-encoder\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Pre-Trained Encoder\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#pre-trained-encoder\",\n    \"aria-label\": \"pre trained encoder permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The pre-trained encoder is quite simple, PyTorch has models available that are pre-trained on ImageNet so all we need to do is call an instance of the network.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"import torchvision.models as models\\n\\ninception = models.inception_v3(pretrained=True)\\n\")), mdx(\"h2\", {\n    \"id\": \"fusion\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Fusion\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#fusion\",\n    \"aria-label\": \"fusion permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The fusion layer combines the features from the base encoder and the pre-trained encoder into a single input for the decoder network.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class ImgFusion(nn.Module):\\n\\n    def __init__(self):\\n        super(ImgFusion, self).__init__()\\n        # In practice nothing here\\n\\n    def forward(self, img1, img2):\\n\\n        img2 = torch.stack([torch.stack([img2],dim=2)],dim=3)\\n        img2 = img2.repeat(1, 1, img1.shape[2], img1.shape[3])\\n        return torch.cat((img1, img2),1)\\n\")), mdx(\"h2\", {\n    \"id\": \"decoder\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Decoder\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#decoder\",\n    \"aria-label\": \"decoder permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The decoder is very similar to the encoder but in reverse, the number of channels reduces as the feature map sizes increase through the upsampling layers. This is where the encoded features from the base encoder and pre-trained encoder post fusion are used to estimate the AB channels.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class ImgDecoder(nn.Module):\\n\\n    def __init__(self):\\n        super(ImgDecoder, self).__init__()\\n\\n        self.layers = nn.Sequential(\\n\\n            # Conv1\\n            nn.Conv2d(256, 128, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(128),\\n\\n            # Upsample1\\n            nn.Upsample(scale_factor=2.0),\\n\\n            # Conv2\\n            nn.Conv2d(128, 64, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(64),\\n\\n            # Conv3\\n            nn.Conv2d(64, 64, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(64),\\n\\n            # Upsample2\\n            nn.Upsample(scale_factor=2.0),\\n\\n            # Conv4\\n            nn.Conv2d(64, 32, 3, stride=1, padding=1),\\n            nn.ReLU(inplace=True),\\n            nn.BatchNorm2d(32),\\n\\n            # Conv5\\n            nn.Conv2d(32, 2, 3, stride=1, padding=1),\\n            nn.Tanh(),\\n\\n            # Upsample3\\n            nn.Upsample(scale_factor=2.0),\\n        )\\n\\n    def forward(self, x):\\n        return self.layers(x)\\n\")), mdx(\"h2\", {\n    \"id\": \"whole-network\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Whole Network\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#whole-network\",\n    \"aria-label\": \"whole network permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Lastly, we just need to connect all the prior blocks into our whole network!\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class ColorNet(nn.Module):\\n\\n    def __init__(self):\\n        super(ColorNet, self).__init__()\\n        self.encoder = ImgEncoder()\\n        self.fusion = ImgFusion()\\n        self.decoder = ImgDecoder(256)\\n        self.post_fuse = nn.Conv2d(1256, 256, 1, stride=1, padding=0)\\n        self.relu = nn.ReLU(inplace=True)\\n\\n    def forward(self, img1, img2):\\n\\n        # Encoder Output\\n        out_enc = self.encoder(img1)\\n\\n        # Fusion\\n        temp = self.fusion(out_enc, img2)\\n        temp = self.post_fuse(temp)\\n        temp = self.relu(temp)\\n\\n        return self.decoder(temp)\\n\")), mdx(\"h1\", {\n    \"id\": \"training\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Training\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#training\",\n    \"aria-label\": \"training permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Here I will go over decisions made for training, results of training, and some issues I came across.\"), mdx(\"h2\", {\n    \"id\": \"optimizer\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Optimizer\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#optimizer\",\n    \"aria-label\": \"optimizer permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"I chose to use the Adam optimizer with the following parameters:\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Parameter\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Value\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Learning Rate\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.0012\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Weight Decay\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"1e-6\")))), mdx(\"p\", null, \"In PyTorch that looks like this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"import torch.optim as optim\\noptimizer = optim.Adam(model.parameters(), lr=0.0012, weight_decay=1e-6)\\n\")), mdx(\"h2\", {\n    \"id\": \"criterion\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Criterion\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#criterion\",\n    \"aria-label\": \"criterion permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As we will be comparing the expected AB results with the ground truth we can use a simple MSE loss for this problem.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"criterion = nn.MSELoss()\\n\")), mdx(\"h2\", {\n    \"id\": \"training-loop\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Training Loop\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#training-loop\",\n    \"aria-label\": \"training loop permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Nothing too special here. Given the prior dataloaders we made it's easy to enumerate over the loader and proceed with moving data to the GPU, running the forward pass, computing loss, and run an optimizer step. Sadly, given my hardware limitation I was not able to compute the validation loss during training which made backtracking to the best model impossible, I just had to assume training was at a reasonable point from the training loss.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"epochs = 50\\n\\nfor i in range(epochs):\\n\\n    train_total_loss = 0\\n    val_total_loss = 0\\n    print(\\\"Epoch:\\\", i+1)\\n\\n    model.train()\\n\\n    # Train\\n    for i,data in enumerate(train_loader,0):\\n\\n        # Move Data to GPU\\n        enc_in = data[\\\"L_enc\\\"].cuda()\\n        inc_in = data[\\\"L_inc\\\"].cuda()\\n        AB = data[\\\"AB\\\"].cuda()\\n\\n        # Init. Optim. Params.\\n        optimizer.zero_grad()\\n\\n        # Forward Prop.\\n        # Get Inception Output\\n        out_incept = inception(inc_in)\\n        # Get Network AB\\n        net_AB = model(enc_in, out_incept)\\n\\n        # Determine Loss\\n        loss = criterion(net_AB, AB)\\n\\n        # Back Prop.\\n        loss.backward()\\n\\n        # Update Weights\\n        optimizer.step()\\n\\n        # Update Loss Saves\\n        train_batch_loss.append(loss.item())\\n        train_total_loss += loss.item()\\n\\n    train_epoch_loss.append(train_total_loss)\\n\\n    # Print Info Every Epoch\\n    print(\\\"Train Loss: \\\", train_total_loss)\\n    # print(\\\"Val. Loss: \\\", val_total_loss)\\n\")), mdx(\"h2\", {\n    \"id\": \"loss-results\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Loss Results\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#loss-results\",\n    \"aria-label\": \"loss results permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The training loss was very interesting for this model, a shape I've never seen before. In hindsight, this seems like too large of a learning rate but I didn't know better back then.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/20e71d7c28d6860ab8f38edffa6e5013/8b34c/training.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"66.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBAgX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB3iq0eQL/AP/EABkQAAMAAwAAAAAAAAAAAAAAAAABAhARMf/aAAgBAQABBQLNSmaFz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABgQAAIDAAAAAAAAAAAAAAAAAAEQIDGR/9oACAEBAAY/AoWdX//EABkQAQEAAwEAAAAAAAAAAAAAAAEAEBEhUf/aAAgBAQABPyE7aMIbSPAzif/aAAwDAQACAAMAAAAQ38//xAAVEQEBAAAAAAAAAAAAAAAAAAAQIf/aAAgBAwEBPxCn/8QAFREBAQAAAAAAAAAAAAAAAAAAECH/2gAIAQIBAT8Qh//EABsQAQADAAMBAAAAAAAAAAAAAAEAETEhQZGh/9oACAEBAAE/EFZ2BPB9hAVpMRRPJfYMZqoaC1rt1n//2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Training Loss\",\n    \"title\": \"Training Loss\",\n    \"src\": \"/static/20e71d7c28d6860ab8f38edffa6e5013/6c738/training.jpg\",\n    \"srcSet\": [\"/static/20e71d7c28d6860ab8f38edffa6e5013/73b64/training.jpg 300w\", \"/static/20e71d7c28d6860ab8f38edffa6e5013/3ad8d/training.jpg 600w\", \"/static/20e71d7c28d6860ab8f38edffa6e5013/6c738/training.jpg 1200w\", \"/static/20e71d7c28d6860ab8f38edffa6e5013/8b34c/training.jpg 1800w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Training Loss\"), \"\\n  \")), mdx(\"p\", null, \"At the end of training a checkpoint is saved for us in inference later on.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"checkpoint = {'model': ColorNet(),\\n          'state_dict': model.state_dict(),\\n          'optimizer' : optimizer.state_dict()}\\n\\ntorch.save(checkpoint, 'checkpoint.pth')\\n\")), mdx(\"p\", null, \"And a little helper function to retrieve the model back.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def load_checkpoint(filepath):\\n    checkpoint = torch.load(filepath)\\n    model = checkpoint['model']\\n    model.load_state_dict(checkpoint['state_dict'])\\n    for parameter in model.parameters():\\n        parameter.requires_grad = False\\n\\n    model.eval()\\n    return model\\n\\nmodel = load_checkpoint('checkpoint.pth')\\n\")), mdx(\"p\", null, \"After that we can test some images!\"), mdx(\"h1\", {\n    \"id\": \"results\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Results\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#results\",\n    \"aria-label\": \"results permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Below we see a set of sample images from the test set that were passed through the model. The left column are the input grayscale images, the center are the predicted images, and the right side are the original images. As we can see, this hardly passes the human eye but the performance isn't too bad.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/60ff02089584c3414a376aaad60e89fd/67f21/collage_test.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"100%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIEA//EABYBAQEBAAAAAAAAAAAAAAAAAAEAAv/aAAwDAQACEAMQAAAB3SiNAaYMvYN//8QAGxABAAICAwAAAAAAAAAAAAAAAgEDBBMSIiP/2gAIAQEAAQUCIWwh8ayoIPoTGquOppMWHHMCuqCf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAgEBPwEf/8QAIBAAAQQBBAMAAAAAAAAAAAAAAAECESFREyMzQVKBof/aAAgBAQAGPwJVnPY6+vItfo5dXNQO3pSMHJPoVZWxzZdZEqf/xAAcEAADAQACAwAAAAAAAAAAAAAAAREhMUFhcaH/2gAIAQEAAT8hS9URXvppA/c18xCURu0SikmG9dJYneQtda+mBJJyiWM3s//aAAwDAQACAAMAAAAQL9+8/8QAFxEBAAMAAAAAAAAAAAAAAAAAAQAQEf/aAAgBAwEBPxBJl//EABcRAAMBAAAAAAAAAAAAAAAAAAABERD/2gAIAQIBAT8QThd//8QAHhABAQACAgMBAQAAAAAAAAAAAREAITFhQVFxgfD/2gAIAQEAAT8QSrnsTd5L8+YCOick2d684YIpv4GNCzcEYn3WIMoSgvZ9+ZlSHEgQhrj+uKtI0UhDZrvC8h0tAo611kGraoXg6z//2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Test Set Images\",\n    \"title\": \"Test Set Images\",\n    \"src\": \"/static/60ff02089584c3414a376aaad60e89fd/6c738/collage_test.jpg\",\n    \"srcSet\": [\"/static/60ff02089584c3414a376aaad60e89fd/73b64/collage_test.jpg 300w\", \"/static/60ff02089584c3414a376aaad60e89fd/3ad8d/collage_test.jpg 600w\", \"/static/60ff02089584c3414a376aaad60e89fd/6c738/collage_test.jpg 1200w\", \"/static/60ff02089584c3414a376aaad60e89fd/8b34c/collage_test.jpg 1800w\", \"/static/60ff02089584c3414a376aaad60e89fd/111a0/collage_test.jpg 2400w\", \"/static/60ff02089584c3414a376aaad60e89fd/67f21/collage_test.jpg 2668w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Test Set Images\"), \"\\n  \")), mdx(\"p\", null, \"It's interesting to start to characterize the performance. For example, consider the first row images of the building. The model does quite well on the sky and building color as buildings don't come in too many colors so it's easier to be more confident about what the color is. On the other hand, people's clothing greatly vary in color and so it's much more difficult to tell what color they should be.\"), mdx(\"h2\", {\n    \"id\": \"loss-problem\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Loss Problem\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#loss-problem\",\n    \"aria-label\": \"loss problem permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The MSE loss promotes predicting unsaturated colors for objects that are harder to tell. An intuitive way to think about this is considering two objects that look identical in grayscale bt vary in true color. The model could choose to be confident and choose a saturated color but if it's wrong it's VERY wrong. So to minimize MSE, the model is conservative and chooses median colors to represent objects of ambiguity. We see this as a fairly vibrant sky whereas the people are drab and gray.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/44866/people.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"51.33333333333333%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAF6N8iYpK//xAAaEAACAgMAAAAAAAAAAAAAAAABAgMQERMy/9oACAEBAAEFAplG8KcwcX//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAaEAACAgMAAAAAAAAAAAAAAAAAEQEgIUJx/9oACAEBAAY/AnlmxPaf/8QAGhAAAgMBAQAAAAAAAAAAAAAAAREAITEgcf/aAAgBAQABPyEg9SwFKNEjFUY9j9cf/9oADAMBAAIAAwAAABA7z//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABURAQEAAAAAAAAAAAAAAAAAABAR/9oACAECAQE/EIf/xAAcEAEAAwACAwAAAAAAAAAAAAABABExECFBUXH/2gAIAQEAAT8QEyaK3FMK8wz6B4fYAQPejeHFHolE/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Gray People\",\n    \"title\": \"Gray People\",\n    \"src\": \"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/6c738/people.jpg\",\n    \"srcSet\": [\"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/73b64/people.jpg 300w\", \"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/3ad8d/people.jpg 600w\", \"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/6c738/people.jpg 1200w\", \"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/8b34c/people.jpg 1800w\", \"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/111a0/people.jpg 2400w\", \"/static/1cb8e63fc1d28edc0f0ea6bb8bd84313/44866/people.jpg 2484w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Gray People\"), \"\\n  \")), mdx(\"p\", null, \"Another way to think about this is to consider what are the possible colors for a given object, we see those that are well-defined are more vibrant whereas others are not as vibrant.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"864px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/5bfe644c8817a96aa810074db19ebc9f/c47f4/color_table.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"37%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB2AWB/8QAFRABAQAAAAAAAAAAAAAAAAAAAAH/2gAIAQEAAQUCqI//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAWEAADAAAAAAAAAAAAAAAAAAAAEDH/2gAIAQEABj8CIv/EABgQAAMBAQAAAAAAAAAAAAAAAAABETGh/9oACAEBAAE/Ia6xmtowf//aAAwDAQACAAMAAAAQcA//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAEAAgIDAAAAAAAAAAAAAAABAEERITFhcf/aAAgBAQABPxA4OAq8qCuhWyIeAdT/2Q==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Object Colors\",\n    \"title\": \"Object Colors\",\n    \"src\": \"/static/5bfe644c8817a96aa810074db19ebc9f/c47f4/color_table.jpg\",\n    \"srcSet\": [\"/static/5bfe644c8817a96aa810074db19ebc9f/73b64/color_table.jpg 300w\", \"/static/5bfe644c8817a96aa810074db19ebc9f/3ad8d/color_table.jpg 600w\", \"/static/5bfe644c8817a96aa810074db19ebc9f/c47f4/color_table.jpg 864w\"],\n    \"sizes\": \"(max-width: 864px) 100vw, 864px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Object Colors\"), \"\\n  \")), mdx(\"p\", null, \"Either way the solution is a loss that allows for many possibilities for a given object as opposed to dictating a single correct solution. Such losses exist and are detailed \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/1603.08511v5.pdf\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"here\"), \" but I did not implement it.\"), mdx(\"h1\", {\n    \"id\": \"conclusion\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Conclusion\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#conclusion\",\n    \"aria-label\": \"conclusion permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As always let me know if you have any questions or suggestions but I hope you enjoyed the post!\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Deep Image Colorization","date":"March 20th, 2021","excerpt":"A summary on my solution to the grayscale image colorization problem using convolutional neural networks.","featureImage":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBP/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHXQJCf/8QAGBABAAMBAAAAAAAAAAAAAAAAAgEDISP/2gAIAQEAAQUCFfQCYJrz/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8BR//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAECAQE/AVf/xAAYEAACAwAAAAAAAAAAAAAAAAAAcQIQEf/aAAgBAQAGPwLWTVf/xAAcEAACAgIDAAAAAAAAAAAAAAABEQAhMUFRYXH/2gAIAQEAAT8hQdX29loC1zDAmd8z/9oADAMBAAIAAwAAABBz/wD/xAAWEQADAAAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8QiZB//8QAFhEAAwAAAAAAAAAAAAAAAAAAAAER/9oACAECAQE/EK0Uf//EABkQAQACAwAAAAAAAAAAAAAAAAEAESExQf/aAAgBAQABPxBdGFKW04lhWEMnpKoFU4c//9k="},"images":{"fallback":{"src":"/static/611862dd82f5ddfdc0151dc1e94269c0/3b7e5/collage.jpg","srcSet":"/static/611862dd82f5ddfdc0151dc1e94269c0/b17b0/collage.jpg 200w,\n/static/611862dd82f5ddfdc0151dc1e94269c0/4fc49/collage.jpg 400w,\n/static/611862dd82f5ddfdc0151dc1e94269c0/3b7e5/collage.jpg 800w,\n/static/611862dd82f5ddfdc0151dc1e94269c0/9f2cc/collage.jpg 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[]},"width":800,"height":237}}}},"fields":{"path":"/projects/image-colorization/","readingTime":{"text":"14 min read"}},"tableOfContents":{"items":[{"url":"#introduction","title":"Introduction","items":[{"url":"#model-overview","title":"Model Overview"}]},{"url":"#problem-statement","title":"Problem Statement"},{"url":"#dataset","title":"Dataset"},{"url":"#model-construction","title":"Model Construction","items":[{"url":"#base-encoder","title":"Base Encoder"},{"url":"#pre-trained-encoder","title":"Pre-Trained Encoder"},{"url":"#fusion","title":"Fusion"},{"url":"#decoder","title":"Decoder"},{"url":"#whole-network","title":"Whole Network"}]},{"url":"#training","title":"Training","items":[{"url":"#optimizer","title":"Optimizer"},{"url":"#criterion","title":"Criterion"},{"url":"#training-loop","title":"Training Loop"},{"url":"#loss-results","title":"Loss Results"}]},{"url":"#results","title":"Results","items":[{"url":"#loss-problem","title":"Loss Problem"}]},{"url":"#conclusion","title":"Conclusion"}]}},"site":{"siteMetadata":{"title":"Patrick Youssef"}}},"pageContext":{"post_id":"/projects/image-colorization/"}},"staticQueryHashes":[]}