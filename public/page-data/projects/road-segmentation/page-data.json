{"componentChunkName":"component---src-templates-blog-post-js","path":"/projects/road-segmentation/","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"templateKey\": \"project\",\n  \"published\": true,\n  \"title\": \"Roadway Segmentation\",\n  \"slug\": \"road-segmentation\",\n  \"date\": \"2021-06-05T00:00:00.000Z\",\n  \"featureImage\": \"images/road_background.jpg\",\n  \"pinned\": true,\n  \"tags\": [\"Deep Nets\", \"Transfer Learning\"],\n  \"excerpt\": \"Reviewing some of the foundational techniques used for semantic segmentation using Convolutional Neural Networks (CNNs) and implementing one variant using a U-Net with transfer learning.\"\n};\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\nvar PostButton = makeShortcode(\"PostButton\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"report\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Report\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#report\",\n    \"aria-label\": \"report permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"You can find more details in the report that was submitted for this work.\"), mdx(PostButton, {\n    text: \"Report\",\n    target: \"/reports/road_segmentation.pdf\",\n    mdxType: \"PostButton\"\n  }), mdx(\"h1\", {\n    \"id\": \"introduction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Introduction\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#introduction\",\n    \"aria-label\": \"introduction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"In this post we'll go over some foundational techniques used for semantic segmentation using Convolutional Neural Networks (CNNs). After covering some recent, we will implement a well-know model and present results and improvements. In particular we will implement a U-Net model and improve the performance modern techniques such as batch norm and transfer learning. At the end of the post I will train the model on the CityScapes dataset and present the results.\"), mdx(\"h1\", {\n    \"id\": \"problem-statement\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Problem Statement\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#problem-statement\",\n    \"aria-label\": \"problem statement permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Semantic segmentation is the problem of predicting classes for each pixel of an input image. Given an image of $\", \"[H, W, C]\", \"$ (where $C$ is the number of channels) the goal is predict class scores over each pixel. The result is a matrix of size $\", \"[H, W, \\\\text{N}_\\\\text{classes}]\", \"$. To convert the class scores into a resulting segmentation of size $\", \"[H, W, 1]\", \"$ we assign the class with the highest score to each pixel using \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"argmax\"), \" along the class dimension. At this point we have a format that is similar to the ground truth for supervised learning. Generally speaking our model is a function such that:\"), mdx(\"p\", null, \"$$\\nf(\\\\text{Image} \", \"[H, W, C]\", \") \\\\rightarrow \\\\text{Class Scores} \\\\, \", \"[H, W, \\\\text{N}_\\\\text{classes}]\", \"\\n$$\"), mdx(\"h1\", {\n    \"id\": \"dataset-and-preparation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Dataset and Preparation\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#dataset-and-preparation\",\n    \"aria-label\": \"dataset and preparation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As one of the common uses of semantic segmentation is in the autonomous vehicle industry I thought it would be fitting to use a dataset associated with that. I opted on using the finely segmented set of images from the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.cityscapes-dataset.com/\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"CityScapes\"), \", a set of 5000 images segmented across 30 classes. I had to make some changes for the architecture I was using as well as hardware limitation during training.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/24a992d3c6e5950f1209a8c83f288105/054ef/zuerich00.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"49.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsSAAALEgHS3X78AAACQklEQVQozz2L/UsaYQDH77f9tBg0gtgY9LoaVqtY2nPenRZFm71naS+a9/Lc89w953mmTBu96MCNUaE1q5m0Ela0YaRrxmz9tP1jwxqDLx/48v1+KCO9vJr2ocMyPr5Bx7/J8Q0+vMaH1/JBWcr8FDPllVxe3T+ZTpbnN849O0VjLx/YDXtWTVPBBmptd2g9ZXamfi1uf8OphCt1OZf8Ppe89Hws8emCd//P1pmQ/fpC3fu8mDp3b1zIG0e+rYwnbpsONVDSskOPO2ZiEbLpVd4wDjE4oUUGoNHnU0e9/GiQfDiwnZ7XhBKd4ltjdmWKxB3uyPDk0lNnsI6aUS08doy7ns8vMsLLV2432+gcvDc4WT00WTMyzc4wKNy7Gu33j42FXeMLpGE+ZHEZzRPGY6fRRNVbvMrAeKDOMfvMc9ICtBY306fVA7H6CQF21NFjPKgN1Fb7V8wjue6pTyNrm4H1JIrt8rE0H6OGu19H7aGsyZ/pjhWb+Vz7+yi3jBkiABKw6a2P/A+r1PtV6kIXzncmr5uuCuZ8qa1Qrv9RaixQX9jsqS132Va4ai+W2i62e3Z4u4Q5FXMK4hSZUyCDRaus0LoOliLm6LuuxE5H6siUzbeeUZpNJzaNcJrKaQpLSJ8usxUBWpFkRTKD7wJZJHGIZ6GX4b1WgachBJiCHIYcllhZZCCyq9iuSVYs3ZoSXSFk/lXIYMSqMqNINBIA9AGJgqzyfyP9euVKVzSRlu8iAMj3SiKQRQAFAAX6lgDyAP4Fe8fql8xzUwYAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Example from finely segmented set.\",\n    \"title\": \"Example from finely segmented set.\",\n    \"src\": \"/static/24a992d3c6e5950f1209a8c83f288105/8537d/zuerich00.png\",\n    \"srcSet\": [\"/static/24a992d3c6e5950f1209a8c83f288105/eed55/zuerich00.png 300w\", \"/static/24a992d3c6e5950f1209a8c83f288105/7491f/zuerich00.png 600w\", \"/static/24a992d3c6e5950f1209a8c83f288105/8537d/zuerich00.png 1200w\", \"/static/24a992d3c6e5950f1209a8c83f288105/d2cc9/zuerich00.png 1800w\", \"/static/24a992d3c6e5950f1209a8c83f288105/054ef/zuerich00.png 2040w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Example from finely segmented set.\"), \"\\n  \")), mdx(\"h2\", {\n    \"id\": \"size-reduction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Size Reduction\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#size-reduction\",\n    \"aria-label\": \"size reduction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The original images were not only not square but quite large for the hardware I was training on. Due to these constraints I center-cropped a square segment of every images and resized them to $\", \"[128 \\\\times 128]\", \"$. Even with these changes I could barely train up to a batch size of 32, this is something I want to revisit as I get access to better hardware (most importantly more VRAM).\"), mdx(\"h2\", {\n    \"id\": \"class-reduction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Class Reduction\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#class-reduction\",\n    \"aria-label\": \"class reduction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"With the modified input images I also opted to reduce the classes to more general classes. I opted to reduce classes to their associated group, as an example: cars, trucks, bus, etc. were condensed to a vehicle class. This reduced the 30 classes to just 8.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/38cc7a12beaeff0e1da439fc08f4312c/a9b49/class_table.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"31.666666666666664%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAGABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdqAXA//xAAXEAADAQAAAAAAAAAAAAAAAAAAAREh/9oACAEBAAEFAoJaf//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAABD/2gAIAQEABj8Cf//EABgQAQEAAwAAAAAAAAAAAAAAAAEAESFh/9oACAEBAAE/IdBzdEF//9oADAMBAAIAAwAAABCAD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABkQAQADAQEAAAAAAAAAAAAAAAEAETEhkf/aAAgBAQABPxBthfXHIK0fUFMCf//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Class reduction from report.\",\n    \"title\": \"Class reduction from report.\",\n    \"src\": \"/static/38cc7a12beaeff0e1da439fc08f4312c/6c738/class_table.jpg\",\n    \"srcSet\": [\"/static/38cc7a12beaeff0e1da439fc08f4312c/73b64/class_table.jpg 300w\", \"/static/38cc7a12beaeff0e1da439fc08f4312c/3ad8d/class_table.jpg 600w\", \"/static/38cc7a12beaeff0e1da439fc08f4312c/6c738/class_table.jpg 1200w\", \"/static/38cc7a12beaeff0e1da439fc08f4312c/a9b49/class_table.jpg 1584w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Class reduction from report.\"), \"\\n  \")), mdx(\"h1\", {\n    \"id\": \"model-construction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Model Construction\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#model-construction\",\n    \"aria-label\": \"model construction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As mentioned before, I decided to use a U-Net for the primary model architecture. Using PyTorch, I implemented the core components of the model. I modified the encoder by using pre-trained VGG19 layers trained on ImageNet. Part of this project was comparing the results of the base model with random initialization and implementing the pre-trained layers.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/96f5e828c1e83c930aa4a2bb91e64c10/a3423/u-net-architecture.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"66.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsTAAALEwEAmpwYAAABkklEQVQoz4VSyW7bMBTk//9Jb0UPvTQ9+JBLGriGC3lpbEuyQknUSpuLHvdCVtwmSN0OwMsjZt7M4KEQggRdtCz8D845rbU21jnv/fhQCB60SQjz3t+iTR8CdNWLmorfc5Tn+fYp2xxP1lp1A6CU0UqA2SY0qViOMSHVAICkFGlWrePuX2QAawxlcrHCUVwe4kNd14xzFELo6HmVVv522inQicui7ZueOedebF8KUwUmCuB25pHdyqaR9djcpa2xsEm1btliR/7a2XUt/HyuuXqz4IW8fz59uk+d8+IdOOcahmN5/vDlqaHDJDex0KRRdWwRJf4a5r3nhrcJIda+sYYuSuOoKLu7h3ST0eAtXKG1ss5G++7rfN9L4aynJ5CDYUwpZf+Qy17MHpMfG6y0lgMYoxXAMCguh1m0na+JBKuVKXCT4ypLS8EH9NqGVvphmX2c7e6/p1LKhorPj7u7b8f5vpjuzHuH402FkzpPnTXoVaveuNHDmcvluoiSdnVolilRxobgp7RgTEwJ4fSsxiP9BaLF7EXF8Wk+AAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"U-Net Architecture\",\n    \"title\": \"U-Net Architecture\",\n    \"src\": \"/static/96f5e828c1e83c930aa4a2bb91e64c10/8537d/u-net-architecture.png\",\n    \"srcSet\": [\"/static/96f5e828c1e83c930aa4a2bb91e64c10/eed55/u-net-architecture.png 300w\", \"/static/96f5e828c1e83c930aa4a2bb91e64c10/7491f/u-net-architecture.png 600w\", \"/static/96f5e828c1e83c930aa4a2bb91e64c10/8537d/u-net-architecture.png 1200w\", \"/static/96f5e828c1e83c930aa4a2bb91e64c10/a3423/u-net-architecture.png 1555w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"U-Net Architecture\"), \"\\n  \")), mdx(\"h2\", {\n    \"id\": \"contraction-block\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Contraction Block\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#contraction-block\",\n    \"aria-label\": \"contraction block permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The contraction, or encoder, steps of the U-Net compose the left half of the model where the input image is broken down into multi-level features that are saved to be concatenated alongside the expansion, or decoder, steps. Each level of contraction is implemented as a PyTorch module API block that returns the intermediate features at each step.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class ContractionBlock(nn.Module):\\n\\n    def __init__(self, c_in, c_out, pres=None):\\n        super(ContractionBlock, self).__init__()\\n\\n        self.act = nn.ReLU(inplace=True)\\n        self.norm1 = nn.BatchNorm2d(c_out)\\n        self.norm2 = nn.BatchNorm2d(c_out)\\n        self.pretrained = False\\n        self.drop = nn.Dropout2d(0.3)\\n\\n        # If you give pretrained layer used them!\\n        # Otherwise default U-Net layers\\n        if pres is not None:\\n            self.pretrained = True\\n            self.layers = pres\\n        else:\\n            self.conv1 = nn.Conv2d(c_in, c_out, kernel_size=3, padding=1)\\n            self.conv2 = nn.Conv2d(c_out, c_out, kernel_size=3, padding=1)\\n\\n    def forward(self, x):\\n\\n        if self.pretrained:\\n            # Pretrained layers already have BN!\\n            x = self.layers(x)\\n        else:\\n            x = self.conv1(x)\\n            x = self.act(x)\\n            x = self.drop(x)\\n            x = self.norm1(x)\\n            x = self.conv2(x)\\n            x = self.act(x)\\n            x = self.drop(x)\\n            x = self.norm2(x)\\n\\n        return x\\n\")), mdx(\"p\", null, \"The code determines whether or not pre-trained layers are to be used or to initialize with new layers. Essentially if the argument \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"pres\"), \" contains layers they will be used.\"), mdx(\"h2\", {\n    \"id\": \"expansion-block\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Expansion Block\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#expansion-block\",\n    \"aria-label\": \"expansion block permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The expansion block is very similar in principle to the contraction blocks but without pre-trained layers and of course serving a different purpose as they are decoding the input features. Here the results of the prior decoder level and saved encoder features are concatenated together along with continued forward propagation.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class ExpansionBlock(nn.Module):\\n\\n    def __init__(self, c_in, c_out):\\n        super(ExpansionBlock, self).__init__()\\n\\n        self.act = nn.ReLU(inplace=True)\\n        self.norm1 = nn.BatchNorm2d(c_out)\\n        self.norm2 = nn.BatchNorm2d(c_out)\\n        self.norm3 = nn.BatchNorm2d(c_out)\\n        self.drop = nn.Dropout2d(0.3)\\n        self.up = nn.ConvTranspose2d(c_in, c_out, kernel_size=2, stride=2)\\n        self.conv1 = nn.Conv2d(c_in, c_out, kernel_size=3, padding=1)\\n        self.conv2 = nn.Conv2d(c_out, c_out, kernel_size=3, padding=1)\\n\\n    def forward(self, x, save):\\n\\n        # Upsample\\n        # Tranpose convolution to increase feature map size\\n        x = self.up(x)\\n        x = self.act(x)\\n        x = self.drop(x)\\n        x = self.norm1(x)\\n\\n        # Concatenate\\n        # Combine saved encoder results with current upsampling\\n        x = torch.cat((save, x), 1)\\n\\n        # More Convs\\n        # Apply convolutions to merged data for further feature reconstruction\\n        x = self.conv1(x)\\n        x = self.act(x)\\n        x = self.drop(x)\\n        x = self.norm2(x)\\n        x = self.conv2(x)\\n        x = self.act(x)\\n        x = self.drop(x)\\n        x = self.norm3(x)\\n\\n        return x\\n\")), mdx(\"h2\", {\n    \"id\": \"combining-components\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Combining Components\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#combining-components\",\n    \"aria-label\": \"combining components permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"With the helper functions, all we need to do now is to initialize and compose each level of both the encoder and decoder.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class UNet(nn.Module):\\n\\n    def __init__(self):\\n        super(UNet, self).__init__()\\n\\n        # Add Convs\\n        # Used to raise channel count to 1024 for final contraction level\\n        # after VGG pretrained\\n        self.add_conv = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\\n        init_weights(self.add_conv)\\n\\n        # Pool\\n        # Only one needed, no trainable params\\n        self.pool = nn.MaxPool2d(2)\\n\\n        # Contraction\\n        # Blocks with a third argument are the pretrained blocks\\n        # Init weights for all other blocks\\n        self.c1 = ContractionBlock(3,64, pre[0:6])\\n        self.c2 = ContractionBlock(64,128, pre[7:13])\\n        self.c3 = ContractionBlock(128,256, pre[14:26])\\n        self.c4 = ContractionBlock(256,512, pre[27:39])\\n        self.c5 = ContractionBlock(512,1024, pre[40:52])\\n        init_weights(self.c5)\\n\\n        # Expansion\\n        self.e1 = ExpansionBlock(1024,512)\\n        init_weights(self.e1)\\n        self.e2 = ExpansionBlock(512,256)\\n        init_weights(self.e2)\\n        self.e3 = ExpansionBlock(256,128)\\n        init_weights(self.e3)\\n        self.e4 = ExpansionBlock(128,64)\\n        init_weights(self.e4)\\n\\n        # Final Convs\\n        # Convolve final features into raw prediction scores\\n        self.f1 = nn.Conv2d(64, args['n_class'], kernel_size=1)\\n\\n    def forward(self, x):\\n\\n        # Contraction\\n        x1 = self.c1(x)\\n        x = self.pool(x1)\\n        x2 = self.c2(x)\\n        x = self.pool(x2)\\n        x3 = self.c3(x)\\n        x = self.pool(x3)\\n        x4 = self.c4(x)\\n        x = self.pool(x4)\\n        x = self.c5(x)\\n        x = self.add_conv(x)\\n\\n        # Expansion\\n        x = self.e1(x, x4)\\n        x = self.e2(x, x3)\\n        x = self.e3(x, x2)\\n        x = self.e4(x, x1)\\n\\n        # Final Conv\\n        x = self.f1(x)\\n\\n        return x\\n\")), mdx(\"h2\", {\n    \"id\": \"testing-forward-pass\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Testing Forward Pass\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#testing-forward-pass\",\n    \"aria-label\": \"testing forward pass permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"To ensure that the forward pass is working correctly (at least dimensionally) I test an arbitrary input and check the output size.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python{out={2}}\"\n  }, \"# Assemble Entire Model\\nmodel = UNet()\\n\\n# Test on fake input\\nx_test = torch.rand((1,3,128,128))\\nout = model(x_test)\\n\\n# Check Size\\nprint(\\\"Segmentation Mask Size:\\\", out.shape)\\n\\nSegmentation Mask Size: torch.Size([1, 8, 128, 128])\\n\")), mdx(\"p\", null, \"As we can see, the output dimensions are as expected with as many channels as there are classes.\"), mdx(\"h1\", {\n    \"id\": \"training\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Training\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#training\",\n    \"aria-label\": \"training permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"There's nothing too special in my training process but I will give a summary of decisions here.\"), mdx(\"h2\", {\n    \"id\": \"parameters\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Parameters\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#parameters\",\n    \"aria-label\": \"parameters permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"With some informal testing I decided on using the following training parameters:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Adam Optimizer\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Learning Rate: 0.001\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Decay: 0.95/epoch\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Batch Size: 32\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Epochs: 20 (Should be more)\")), mdx(\"p\", null, \"When training, the validation loss is computed on each epoch and if a better loss is found the model is saved in favor of the prior model. All evaluation is done on the final saved model.\"), mdx(\"h2\", {\n    \"id\": \"loss\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Loss\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#loss\",\n    \"aria-label\": \"loss permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"After training both the base and pre-trained models it was easy to compare the loss curves. The pink curve is for the base model whereas the red curve is for the pre-trained model. Moving into evaluation I would expect to see a distinct performance improvement from the pre-trained layers.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"762px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/39b80fb03eaf264706010bd2383279b9/35bc6/ValLosses.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"72.33333333333334%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAAByUlEQVQ4y31Ta5ObMAzk//+95PohJUyv5UIwCQTw+8F2JGLmSHvnmR3Jkr1eCVEMw4Dj8YjD4YCyLFHXNaqqQggBtJZl+QdfxQmFEAKn04lBpOfzGeeqQtu228XX9V2syMwpJUaMEd5Y9kkl7QnZJ/tVjuIFsVrrMI4jYkrw2sA0NyQscNbyYYJzbrN0Oe8/x5gwS+UkqVMGshZIwI7QO/9/Qv9CuCq0mMYJaVngpMLj5/uq8JMK7z0/6LxHWhJb2gf2v1HotET/9gOhm2BqwdCMFubSQX0I2Osd6iJgRQ9d1rBKc7tSfCqUpOrxWBUag+50RDAanpoNIC6Aj/TBEpzzSCHBGYfgAry0cPalZFLJJVFpIWK8dZjfS0x/KshrDdVeINsLVHdlq7sG8/UD+i4gm9+wRvNjG6FSGtM0I6a12VJpUCOMnKFEsyOciVA0mJsa+iYw/SphlXqWHFdCYiaFeaaUlKtqmjnKP8smG3LPaXapOmA9l9K+5HmW/LWVWvtJSsk3WkNrvfn0mDWG7S5nDIva9ZBIiFRKyXs6lGME8vPFnMuxPGLF639Jso2x7A/Dgx+jHo/jxLGuu3Gs7wdovRLd7/12/y9axUKCtnU+4QAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Validation Loss Curves\",\n    \"title\": \"Validation Loss Curves\",\n    \"src\": \"/static/39b80fb03eaf264706010bd2383279b9/35bc6/ValLosses.png\",\n    \"srcSet\": [\"/static/39b80fb03eaf264706010bd2383279b9/eed55/ValLosses.png 300w\", \"/static/39b80fb03eaf264706010bd2383279b9/7491f/ValLosses.png 600w\", \"/static/39b80fb03eaf264706010bd2383279b9/35bc6/ValLosses.png 762w\"],\n    \"sizes\": \"(max-width: 762px) 100vw, 762px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Validation Loss Curves\"), \"\\n  \")), mdx(\"h1\", {\n    \"id\": \"evaluation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Evaluation\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#evaluation\",\n    \"aria-label\": \"evaluation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"I first evaluated the random initialization model and then compared what improvements were to be found using pre-trained encoder layers. I tested a mix of images that were selected as representing some distinct scenes from the dataset. I also computed the mean pixel accuracy and intersection over union.\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Model\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Pixel Acc.\"), mdx(\"th\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"IOU\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Base\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.635\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.243\")), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"Pre-Trained\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.857\"), mdx(\"td\", {\n    parentName: \"tr\",\n    \"align\": null\n  }, \"0.53\")))), mdx(\"p\", null, \"As expected there is a very large difference in the model performance, this trend continues into the test images.\"), mdx(\"h2\", {\n    \"id\": \"base-model-images\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Base Model Images\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#base-model-images\",\n    \"aria-label\": \"base model images permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As we can see the bulk trends are correct but this is not passable as a correct segmentation. This is indicated well by the decent pixel accuracy but terrible intersection over union. The last column indicated where a mistake is made and we can see large regions where the model misclassified the pixel.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/4c6704cfd20c8e1c5288fdce4c8bf148/7124e/val_images_base.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"120.66666666666667%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAYABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAECBAP/xAAWAQEBAQAAAAAAAAAAAAAAAAABAgD/2gAMAwEAAhADEAAAAY5zzZ3qpc9CnWNv/8QAHBAAAgICAwAAAAAAAAAAAAAAAQIREgADEyEi/9oACAEBAAEFAnZuSWzSWq837zQDVgbwZ0jz/8QAFBEBAAAAAAAAAAAAAAAAAAAAIP/aAAgBAwEBPwEf/8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQIBAT8BIT//xAAbEAACAwEBAQAAAAAAAAAAAAAAAQIiMREhMv/aAAgBAQAGPwKV2K47d9J+LTFg+paTrLT5lg6y0//EAB0QAAICAgMBAAAAAAAAAAAAAAARITEBYUFRsZH/2gAIAQEAAT8h6UNakw7Fq2OGXZtI2pb04G+CQmFynMkT3CojY//aAAwDAQACAAMAAAAQXA88/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAxQf/aAAgBAwEBPxDs7BJf/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAEhEfD/2gAIAQIBAT8QSE7BIYf/xAAdEAEBAAIDAQEBAAAAAAAAAAABEQAxIVFhQXGh/9oACAEBAAE/EDkFcWwfH7nvsK7TbiqsVT0McJwGzn3kD8n0nT/MBJNcImjrFoJmo4NZx1gqNDgzq+ZpBqAbIeZ//9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Validation Performance on Base Model\",\n    \"title\": \"Validation Performance on Base Model\",\n    \"src\": \"/static/4c6704cfd20c8e1c5288fdce4c8bf148/6c738/val_images_base.jpg\",\n    \"srcSet\": [\"/static/4c6704cfd20c8e1c5288fdce4c8bf148/73b64/val_images_base.jpg 300w\", \"/static/4c6704cfd20c8e1c5288fdce4c8bf148/3ad8d/val_images_base.jpg 600w\", \"/static/4c6704cfd20c8e1c5288fdce4c8bf148/6c738/val_images_base.jpg 1200w\", \"/static/4c6704cfd20c8e1c5288fdce4c8bf148/8b34c/val_images_base.jpg 1800w\", \"/static/4c6704cfd20c8e1c5288fdce4c8bf148/111a0/val_images_base.jpg 2400w\", \"/static/4c6704cfd20c8e1c5288fdce4c8bf148/7124e/val_images_base.jpg 2655w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Validation Performance on Base Model\"), \"\\n  \")), mdx(\"h2\", {\n    \"id\": \"pre-trained-model-images\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Pre-Trained Model Images\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#pre-trained-model-images\",\n    \"aria-label\": \"pre trained model images permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"This is much better! As we can see the overall performance is much better as objects are decently segmented from their foreground. There are some obvious mistakes but this could be rectified with larger images and more training (both of which were limited by hardware). The error masks are much tighter and we can see errors on the boundaries of classes.\"), mdx(\"p\", null, \"One of the larger mistakes is on detecting humans, I believe this is due to the small size of the image. Small objects will only be represented by a few pixels in the early feature decoding, this makes it more difficult to decode out the small objects and as such they wash out into their bulk surrounding. Clearly, a larger image would greatly improve this aspect of the performance.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/b918116270fe7464a346622afefbab6f/7124e/val_images.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"120.66666666666667%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAYABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAECBAP/xAAWAQEBAQAAAAAAAAAAAAAAAAABAAL/2gAMAwEAAhADEAAAAa0nm52gc9Btgr//xAAeEAACAQMFAAAAAAAAAAAAAAABAgAEERIDEyEiMv/aAAgBAQABBQJ2bcu0p2JR758yn8MDlYzQHT//xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPwEhP//EABsQAAIDAQEBAAAAAAAAAAAAAAABAiIxITJB/9oACAEBAAY/ApXYrjt9JcWmLB8Wkqy08yw8tdP/xAAeEAACAgICAwAAAAAAAAAAAAAAEQEhMVFhcUGBsf/aAAgBAQABPyHShriyqDY5JJmczuxuNb6KnjDXdAseZh3ZKab2FS9of//aAAwDAQACAAMAAAAQ2w/B/8QAFxEBAQEBAAAAAAAAAAAAAAAAAQAxQf/aAAgBAwEBPxDs7BJf/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERIf/aAAgBAgEBPxBMHKJhD//EABwQAQEAAwADAQAAAAAAAAAAAAERACExQVFhcf/aAAgBAQABPxAYBWC2Dw/cnpQmWrO4gBKu7hj2uA6b+8RrR0U9OFouuUThjXEyUaOZr1jTUZKTI65i01LQbw+Z/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Validation Performance on Pre-Trained Model\",\n    \"title\": \"Validation Performance on Pre-Trained Model\",\n    \"src\": \"/static/b918116270fe7464a346622afefbab6f/6c738/val_images.jpg\",\n    \"srcSet\": [\"/static/b918116270fe7464a346622afefbab6f/73b64/val_images.jpg 300w\", \"/static/b918116270fe7464a346622afefbab6f/3ad8d/val_images.jpg 600w\", \"/static/b918116270fe7464a346622afefbab6f/6c738/val_images.jpg 1200w\", \"/static/b918116270fe7464a346622afefbab6f/8b34c/val_images.jpg 1800w\", \"/static/b918116270fe7464a346622afefbab6f/111a0/val_images.jpg 2400w\", \"/static/b918116270fe7464a346622afefbab6f/7124e/val_images.jpg 2655w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Validation Performance on Pre-Trained Model\"), \"\\n  \")), mdx(\"h1\", {\n    \"id\": \"questions\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Questions\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#questions\",\n    \"aria-label\": \"questions permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As mentioned prior, this page serves as a summary of my process and findings. There are many more details in the report but past that if you have more questions feel free to reach out!\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Roadway Segmentation","date":"June 5th, 2021","excerpt":"Reviewing some of the foundational techniques used for semantic segmentation using Convolutional Neural Networks (CNNs) and implementing one variant using a U-Net with transfer learning.","featureImage":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAIE/8QAFAEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAABxhLC/wD/xAAXEAADAQAAAAAAAAAAAAAAAAAAARIR/9oACAEBAAEFAtLRaP/EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAEDAQE/AYj/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQIBAT8BkY//xAAWEAEBAQAAAAAAAAAAAAAAAAAAMSH/2gAIAQEABj8C1Ef/xAAaEAACAgMAAAAAAAAAAAAAAAABEQAhMXGh/9oACAEBAAE/IVBCOWM23ATPU//aAAwDAQACAAMAAAAQfA//xAAXEQEBAQEAAAAAAAAAAAAAAAABABEh/9oACAEDAQE/EAPWwX//xAAWEQEBAQAAAAAAAAAAAAAAAAABABH/2gAIAQIBAT8QBLF//8QAGxABAQADAAMAAAAAAAAAAAAAAREAIVFBYaH/2gAIAQEAAT8QuoTXEgwvMqEJGJHPWF1Fu3y+Z//Z"},"images":{"fallback":{"src":"/static/a28b33372037f3addc9a0ee3701ae9ed/9665f/road_background.jpg","srcSet":"/static/a28b33372037f3addc9a0ee3701ae9ed/34540/road_background.jpg 200w,\n/static/a28b33372037f3addc9a0ee3701ae9ed/a1920/road_background.jpg 400w,\n/static/a28b33372037f3addc9a0ee3701ae9ed/9665f/road_background.jpg 800w,\n/static/a28b33372037f3addc9a0ee3701ae9ed/fd240/road_background.jpg 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[]},"width":800,"height":262}}}},"fields":{"path":"/projects/road-segmentation/","readingTime":{"text":"12 min read"}},"tableOfContents":{"items":[{"url":"#report","title":"Report"},{"url":"#introduction","title":"Introduction"},{"url":"#problem-statement","title":"Problem Statement"},{"url":"#dataset-and-preparation","title":"Dataset and Preparation","items":[{"url":"#size-reduction","title":"Size Reduction"},{"url":"#class-reduction","title":"Class Reduction"}]},{"url":"#model-construction","title":"Model Construction","items":[{"url":"#contraction-block","title":"Contraction Block"},{"url":"#expansion-block","title":"Expansion Block"},{"url":"#combining-components","title":"Combining Components"},{"url":"#testing-forward-pass","title":"Testing Forward Pass"}]},{"url":"#training","title":"Training","items":[{"url":"#parameters","title":"Parameters"},{"url":"#loss","title":"Loss"}]},{"url":"#evaluation","title":"Evaluation","items":[{"url":"#base-model-images","title":"Base Model Images"},{"url":"#pre-trained-model-images","title":"Pre-Trained Model Images"}]},{"url":"#questions","title":"Questions"}]}},"site":{"siteMetadata":{"title":"Patrick Youssef"}}},"pageContext":{"post_id":"/projects/road-segmentation/"}},"staticQueryHashes":[]}