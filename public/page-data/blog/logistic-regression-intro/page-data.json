{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/logistic-regression-intro/","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"templateKey\": \"blog-post\",\n  \"published\": true,\n  \"title\": \"What is Logistic Regression?\",\n  \"slug\": \"logistic-regression-intro\",\n  \"date\": \"2020-12-23T00:00:00.000Z\",\n  \"featureImage\": \"images/Example_Training.png\",\n  \"tags\": [\"Machine Learning\", \"Optimization\"],\n  \"excerpt\": \"How to implement a logistic regression model with a neural network mindset in Python.\",\n  \"pinned\": true\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"introduction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Introduction\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#introduction\",\n    \"aria-label\": \"introduction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"This post will introduce you to the principles behind logistic regression as a binary classification method. Using Python and NumPy we will implement the core principles from scratch on a test dataset. There are different ways to implement this particular algorithm but I will focus on an implementation with a neural network mindset as many of these ideas extend well into neural nets in later posts.\"), mdx(\"h1\", {\n    \"id\": \"what-is-logistic-regression\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"What is Logistic Regression?\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#what-is-logistic-regression\",\n    \"aria-label\": \"what is logistic regression permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Logistic regression is a predictor borrowed from statistics used for binary classification. All this means is that we can use the algorithm to predict whether a given example belongs to a class or not. As an example, if we knew certain features about the weather (temperature, humidity, wind, etc.) we could try to predict if it's going to rain or not. To do so, we need many \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"labeled\"), \" examples of data from other days. We'll get into this more moving forward.\"), mdx(\"h1\", {\n    \"id\": \"math-representation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Math Representation\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#math-representation\",\n    \"aria-label\": \"math representation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Before we can dive into the implementation, we first need to fully understand the required data and math for this problem.\"), mdx(\"h2\", {\n    \"id\": \"data-organization\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Data Organization\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#data-organization\",\n    \"aria-label\": \"data organization permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"We represent a particular example as a vector of features and we store all these examples as one large matrix $X$ where $X_i$ is a particular example (a single day if we follow our prior rain metaphor). The labeled aspect means that we know whether or not that day had rain, we'll call this the ground-truth and save all labels for our examples in a vector $y$ where we store a 0 for the days it didn't rain and a 1 when it does rain.\"), mdx(\"p\", null, \"As an example, let's imagine that we track 2 different aspects to describe each day such as average temperature and humidity. For a year, we would have 365 examples of temperature and humidity stored in our vector $X$ and the whether it rained or not in our vector $y$.\"), mdx(\"p\", null, \"$$\\nX = \", \"[365 \\\\times 2]\", \" =\\n\\\\begin{bmatrix}\\n72 & 0.42\", \"\\\\\", \"\\n55 & 0.92\", \"\\\\\", \"\\n\\\\vdots & \\\\vdots \", \"\\\\\", \"\\n53 & 0.95\", \"\\\\\", \"\\n71 & 0.43\", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"$$\\ny=\", \"[365 \\\\times 1]\", \" =\\n\\\\begin{bmatrix}\\n0\", \"\\\\\", \"\\n1\", \"\\\\\", \"\\n\\\\vdots \", \"\\\\\", \"\\n1\", \"\\\\\", \"\\n0\", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"h2\", {\n    \"id\": \"forward-propagation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Forward Propagation\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#forward-propagation\",\n    \"aria-label\": \"forward propagation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The forward propagation step is where we take an example $X_i$ and pass it through our model to yield a prediction $\\\\hat{y_i}$. To do so, we have a vector $W$ that stores what we call the weights. We also have a bias term that we denote as $b$. Together with the sigmoid function ($\\\\sigma$) we get the following:\"), mdx(\"p\", null, \"$$\\n\\\\hat{y_i} = \\\\sigma (W^T X_i + b)\\n$$\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/38a4ccd3bafe8c515bd9e8e266ec3cb0/111a0/sigmoid.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"66.66666666666666%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3pRQf//EABYQAAMAAAAAAAAAAAAAAAAAABARIP/aAAgBAQABBQKEP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABkQAAMAAwAAAAAAAAAAAAAAAAABERAhYf/aAAgBAQABPyEiETuC0f/aAAwDAQACAAMAAAAQ4A//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAaEAEAAgMBAAAAAAAAAAAAAAABABEhMUFh/9oACAEBAAE/ENrZ2ArgOYxNMRVqIlHvmoKVP//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Sigmoid Function\",\n    \"title\": \"Sigmoid Function\",\n    \"src\": \"/static/38a4ccd3bafe8c515bd9e8e266ec3cb0/6c738/sigmoid.jpg\",\n    \"srcSet\": [\"/static/38a4ccd3bafe8c515bd9e8e266ec3cb0/73b64/sigmoid.jpg 300w\", \"/static/38a4ccd3bafe8c515bd9e8e266ec3cb0/3ad8d/sigmoid.jpg 600w\", \"/static/38a4ccd3bafe8c515bd9e8e266ec3cb0/6c738/sigmoid.jpg 1200w\", \"/static/38a4ccd3bafe8c515bd9e8e266ec3cb0/8b34c/sigmoid.jpg 1800w\", \"/static/38a4ccd3bafe8c515bd9e8e266ec3cb0/111a0/sigmoid.jpg 2400w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Sigmoid Function\"), \"\\n  \")), mdx(\"p\", null, \"Where $\\\\hat{y_i}$ is the probability that the example is in the class we are trying to predict. For the rain example, this is the logistic regressions estimated probability that it will rain that day. The sigmoid function is well catered to this as it introduces a non-linearity that bounds the output between 0 and 1, perfect for a probability!\"), mdx(\"h3\", {\n    \"id\": \"example-forward-propagation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Example Forward Propagation\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#example-forward-propagation\",\n    \"aria-label\": \"example forward propagation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's consider our example $X$ in the prior data organization section. I understand that at this point I have not discussed how to determine $W$ and $b$ but for now follow that we know it, we will discuss how to find the correct values in the next section. Let's use the following:\"), mdx(\"p\", null, \"$$\\nW^T = \", \"[1 \\\\times 2]\", \" =\\n\\\\begin{bmatrix}\\n0.1 & 10\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"$$\\nb = -13\\n$$\"), mdx(\"p\", null, \"We can actually go ahead and bulk propagate all examples using matrix multiplication! Let's use $X$ and $y$ as the define days from before which is:\"), mdx(\"p\", null, \"$$\\nX = \", \"[4 \\\\times 2]\", \" =\\n\\\\begin{bmatrix}\\n72 & 0.42\", \"\\\\\", \"\\n55 & 0.92\", \"\\\\\", \"\\n53 & 0.95\", \"\\\\\", \"\\n71 & 0.43\", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"$$\\ny=\", \"[4 \\\\times 1]\", \" =\\n\\\\begin{bmatrix}\\n0\", \"\\\\\", \"\\n1\", \"\\\\\", \"\\n1\", \"\\\\\", \"\\n0\", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"Following the equation for $\\\\hat{y_i}$ we get the following:\"), mdx(\"p\", null, \"$$\\n\\\\hat{y_i} = \", \"[4 \\\\times 1]\", \" = \\\\sigma (W^T X^T + b)\\n$$\"), mdx(\"p\", null, \"$$\\n\\\\hat{y_i} = \\\\sigma (\\n\\\\begin{bmatrix}\\n0.1 & 10\\n\\\\end{bmatrix}\"), mdx(\"p\", null, \"\\\\begin{bmatrix}\\n72 & 55 & 53 & 71\", \"\\\\\", \"\\n0.42 & 0.92 & 0.95 & 0.43\", \"\\\\\", \"\\n\\\\end{bmatrix}\\n-13\\n)\\n$$\"), mdx(\"p\", null, \"$$\\n\\\\hat{y_i} =\\n\\\\sigma(\\n\\\\begin{bmatrix}\\n-1.6 & 1.7 & 1.8 &-1.6 \", \"\\\\\", \"\\n\\\\end{bmatrix}\\n)\\n$$\"), mdx(\"h1\", {\n    \"id\": \"\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"$$\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#\",\n    \"aria-label\": \" permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"\\\\begin{bmatrix}\\n0.17 & 0.85 & 0.86 & 0.17 \", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"When we compare those probabilities to the known results we can see they match! Days that had rain (days 2 and 3) show high probabilities that it rained and the non rain days respectively show low probabilities. In practice you can get a prediction as a 1 or 0 by rounding the probability, the idea is you round towards the prediction that is more likely in the probability. In the case of logistic regression where there are only two possibilities (rain or didn't) you can estimate one from the other very easily.\"), mdx(\"p\", null, \"$$\\n\\\\text{P}\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"{\\\\text{Clear}} = 1 - \\\\text{P}\"), \"{\\\\text{Rain}}\\n$$\"), mdx(\"p\", null, \"Essentially, if the probability is greater or less than $0.5$ we round as the alternate probability becomes lower.\"), mdx(\"h3\", {\n    \"id\": \"bias-simplification\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Bias Simplification\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#bias-simplification\",\n    \"aria-label\": \"bias simplification permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Personally, I prefer to move the bias term ($b$) into the matrix multiplication ($W^T X_i$). This is done by adding a column of 1's to the end of $X$, adding a feature that is consistent across all examples. Then we increase the size of the weights by 1 (now $W',X'$ etc.) such that the last weight acts as the bias. Check the math below if you are still curious. This helps to simplify the training procedure as you only need to train for $W'$! Moving forward, I will refer to $W'$ as just $W$.\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"$$\\nW^T X + b  = w_1 x_1 + ... + w_n x_n + b\\n$$\"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"$$\\n= w\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"1 x_1 + ... + w_n x_n + w\"), \"{n+1} \", \"[1,1,...,1]\", \"\\n$$\"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"$$\\n= W'X'\\n$$\")), mdx(\"h3\", {\n    \"id\": \"finding-the-correct-w-and-b\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Finding the Correct $W$ and $b$\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#finding-the-correct-w-and-b\",\n    \"aria-label\": \"finding the correct w and b permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As mentioned prior I just gave you suitable values for $W$ and $b$ to go through the example by tweaking with the results in a python session. In practice, it can be difficult/impossible to self determine the correct values. To most correctly determine the values we use a process called training!\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Values of the weights are determined through training!\")), mdx(\"h1\", {\n    \"id\": \"training\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Training\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#training\",\n    \"aria-label\": \"training permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Training is the process where we take many labeled examples and use them to determine values for $w$ that will yield us the best \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"overall\"), \" performance. To be precise, we will describe some formulations for what are called the cost and loss of the model.\"), mdx(\"h2\", {\n    \"id\": \"cost-and-loss-functions\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Cost and Loss Functions\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#cost-and-loss-functions\",\n    \"aria-label\": \"cost and loss functions permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's start with the loss function. The loss is a measure of the error for a particular example's prediction. Following our prior notation the loss for one example is:\"), mdx(\"p\", null, \"$$\\n\\\\mathcal{L} = -y \\\\log(\\\\hat{y}) - (1-y) \\\\log(1-\\\\hat{y})\\n$$\"), mdx(\"p\", null, \"Let's play this out and see why it makes sense. For a given example that we know had no rain ($y=0$), we can consider the error as the log of the difference between the prediction $\\\\hat{y}$ and 0 as the left side term goes to 0 and we're just left with $\\\\log(1-\\\\hat{y})$. As a reminder, $\\\\log(1)=0$ so any deviation from 1 would create an error. Going back to if $y=0$, a perfect prediction would be if $\\\\hat{y}=0$ and we'd be left with $\\\\log(1)=0$. Therefore, the cost function accommodates for the two different possible values of $y$ and takes the log of the difference for the respective cases.\"), mdx(\"p\", null, \"The cost function is easy, it's just the average of the loss functions so:\"), mdx(\"p\", null, \"$$\\nCost = \\\\frac{1}{N}\\\\sum_{i=1}^{N} \\\\mathcal{L}_i\\n$$\"), mdx(\"p\", null, \"Now that we have a formulation for a function to decrease let's just take the derivative and set it equal to 0 to see what the values of $W$ should be. In practice, this is not possible so we need to approach this as a numerical optimization problem.\"), mdx(\"h2\", {\n    \"id\": \"gradient-descent\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Gradient Descent\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#gradient-descent\",\n    \"aria-label\": \"gradient descent permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Gradient descent is fairly simple, given the gradient of a function with respect to the variable to be optimized take a step in the negative direction of the gradient (positive for gradient ascent) and update your inputs with the step. Keep repeating this until you are happy with the convergence. In general, it looks something like this:\"), mdx(\"p\", null, \"$$\\n\\\\theta_{i+1} = \\\\theta_i - \\\\alpha \\\\nabla f(\\\\theta)\\n$$\"), mdx(\"p\", null, \"Where $\\\\alpha$ is a tuneable parameter called the learning rate, it dictates what fraction of the gradient we should step by. I will not go through the derivation (\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/pdfs/40%20LogisticRegression.pdf\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"great resource\"), \"), but for our problem we would like to use gradient descent where $f$ is the cost function and we take the gradient in terms of $W$. For our use case this is:\"), mdx(\"p\", null, \"$$\\nW_{i+1} = W_i - \\\\alpha \\\\frac{1}{N} X^T(\\\\sigma(W^T X^T)-y)^T\\n$$\"), mdx(\"p\", null, \"Now we have everything to implement!\"), mdx(\"h1\", {\n    \"id\": \"implementation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Implementation\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#implementation\",\n    \"aria-label\": \"implementation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's go ahead and import our python modules first as follow:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"import numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.datasets import load_breast_cancer\\n\")), mdx(\"p\", null, \"I will be using Python and NumPy for the implementation. The dataset used is imported in our prior code block with the rest of our libraries. The dataset takes a few human health metrics as features and tries to predict if the patient has breast cancer, you can read more \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"here\"), \".\"), mdx(\"h2\", {\n    \"id\": \"model-class\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Model Class\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#model-class\",\n    \"aria-label\": \"model class permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As we have many parameters, helper functions, and repeated operations I will be creating the model as a Python class. Let's go ahead and initialize the class with all the methods we will be using.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"class LogisticRegression():\\n\\n    def sigmoid(self, z):\\n\\n    def prep_data(self, X):\\n\\n    def add_dimension(self, X):\\n\\n    def train(self, X_train, y_train, learning_rate, iterations):\\n\\n    def predict(self, X_test):\\n\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"sigmoid\"), \" will be where we write the sigmoid function\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"prep_data\"), \" is where we will normalize our data to between 0 and 1\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"add_dimension\"), \" is a helper function for adding the column of 1's to $X$\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"train\"), \" is where we will run the gradient descent iterations to determine our weights\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"inlineCode\", {\n    parentName: \"li\"\n  }, \"predict\"), \" is where we will forward propagate on unseen (test) data to estimate the classes\")), mdx(\"p\", null, \"Let's start with the \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"sigmoid\"), \" function, as mentioned before the function is $(1+\\\\exp(-x))^{-1}$, using NumPy this is:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def sigmoid(self, z):\\n    return 1 / (1 + np.exp(-z))\\n\")), mdx(\"p\", null, \"Let's go ahead to \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"prep_data\"), \" and \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"add_dimension\"), \" as they work together. \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"prep_data\"), \" takes the range of each column and brings them down to $\", \"[0,1]\", \"$, that way no particular feature dominates the result of $W^T X_i$. \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"add_dimension\"), \" creates a vector of ones the size of the number of samples and tacks it as the last column of $X$.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def prep_data(self, X):\\n    temp = X - np.min(X, axis=0)\\n    X = temp / np.max(temp, axis=0)\\n\\n    return X\\n\\ndef add_dimension(self, X):\\n    # Stacks a vector of ones with the same number of rows as X\\n    # horizontally on then end of X\\n    X_new = np.hstack((X, np.ones((X.shape[0], 1))))\\n\\n    return X_new\\n\")), mdx(\"p\", null, \"The training process is going to be more involved, the comments explain what is going on but feel free to reach out if you would like more explanation.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def train(self, X_train, y_train, learning_rate, iterations):\\n    self.learning_rate = learning_rate\\n    self.iterations = iterations\\n\\n    X_train = self.prep_data(X_train)\\n    X_train = self.add_dimension(X_train)\\n\\n    N,D = X_train.shape\\n    self.weights = np.random.random((D,1)) * 0.001\\n\\n    # Iterate to train\\n    for i in range(self.iterations):\\n\\n        # Compute the gradient\\n        A = self.sigmoid(np.dot(self.weights.T, X_train.T))\\n        grad = (1/N) * np.dot(X_train.T, (A - y_train).T)\\n\\n        # Update Weights\\n        self.weights = self.weights - self.learning_rate * grad\\n\\n        # Get Training Accuracy Every 100 Iterations\\n        if i % 100 == 0:\\n            y_est = self.predict(X_train)\\n            acc = np.sum(y_train == y_est) / y_train.size\\n            print(\\\"Training Accuracy on Iteration {}: {:0.4f}\\\".format(i, acc))\\n\")), mdx(\"p\", null, \"To close out the model class, let's implement \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"predict\"), \" that computes the forward propagation of our test data.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def predict(self, X_test):\\n    A = self.sigmoid(np.dot(self.weights.T, X_test.T))\\n    y_est = np.round(A)\\n\\n    return y_est\\n\")), mdx(\"h2\", {\n    \"id\": \"data-import-and-function-calls\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Data Import and Function Calls\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#data-import-and-function-calls\",\n    \"aria-label\": \"data import and function calls permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Assuming you have downloaded the aforementioned dataset, you should make sure you have it in the same folder as your Python file! The following code will import it and split the data into a test and train set. I have not mentioned this yet but this is a good place to talk about it.\"), mdx(\"h3\", {\n    \"id\": \"testtrain-split\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Test/Train Split\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#testtrain-split\",\n    \"aria-label\": \"testtrain split permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Quick digression, a test and train split is a process of splitting your data into two groups based on a fraction (typical is 70/30 for train/test). This is very important to do as testing a model on the data it was trained on is not a good metric of its performance. To put it briefly, a model can learn to fit a training set very well but won't generalize well to new data. This is very bad as all we really care about is how it generalizes. This is as much as you need to know for now, but I would highly recommend you read more \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"here\"), \".\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# Load in data\\ndata = load_breast_cancer(return_X_y=True)\\n\\n# Split into X and y\\nX = data[0]\\ny = data[1]\\n\\n# Test Train Split\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,\\n                                                    random_state=1)\\n\")), mdx(\"h2\", {\n    \"id\": \"running-the-model\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Running the Model\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#running-the-model\",\n    \"aria-label\": \"running the model permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"We've made it! It's time to train our model and see how it does on the testing set! The following code will help you out:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python{out={7}}\"\n  }, \"# Create an instance of our logistic regression class\\nMyModel = LogisticRegression()\\n\\n# Train the model on the training set using a learning rate of 0.1\\n# and 300 iterations\\nMyModel.train(X_train, y_train, 0.1, 300)\\n\\n# Prep test data and get predictions for y_test\\nX_test = MyModel.prep_data(X_test)\\nX_test = MyModel.add_dimension(X_test)\\ny_test_est = MyModel.predict(X_test)\\n\\nTraining Accuracy on Iteration 50: 0.8894\\nTraining Accuracy on Iteration 100: 0.9196\\nTraining Accuracy on Iteration 150: 0.9221\\nTraining Accuracy on Iteration 200: 0.9271\\nTraining Accuracy on Iteration 250: 0.9271\\nTraining Accuracy on Iteration 300: 0.9271\\n\")), mdx(\"p\", null, \"Now all we have to is compute the accuracy and see how we did.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python{out={2}}\"\n  }, \"# (Count of correct predictions) / (Total number of test samples)\\ntest_accuracy = np.sum(y_test == y_test_est) / y_test.size\\nprint(\\\"Test Accuracy: {:0.3f}\\\".format(test_accuracy))\\n\\nTest Accuracy: 0.912\\n\")), mdx(\"p\", null, \"This results in an accuracy of around 91%! Considering the low number of features in the dataset this is a great result.\"), mdx(\"h1\", {\n    \"id\": \"moving-forward\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Moving Forward\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#moving-forward\",\n    \"aria-label\": \"moving forward permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As the intro suggests, this is just an introduction and there is much more that we can do with logistic regression. Just to list a few, here are the topics I am considering in extending the basics of logistic regression.\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Underfitting and overfitting in the model, how do we know when this is happening and how can we mitigate it.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Multinomial logistic regression, let's extend the prediction space from in or not in a class to multiple classes.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Tuning of parameters, what other metrics can we look at to better understand the best values for our hyperparameters.\")));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"What is Logistic Regression?","date":"December 23rd, 2020","excerpt":"How to implement a logistic regression model with a neural network mindset in Python.","featureImage":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAB7CAAAewgFu0HU+AAABvElEQVQ4y41Ti46bMBC8///ARpGa6BIC5hW/sTEPT7Um0HBJ72pp8crAeHdm5wMAYoz4uqZpwjAMuwhhwDiOKX+3COdjBQshwDmXgvKyLHE+n3E+nZBdr2B5jux6QX7LUvR9jyHQRQHee8zznHASoJQSQohUFQVVsS7bD2ikwakUuNQSv4s7PiuBw2cOVtco6gZcKoyPqhOgtRbGmF35xnnkjIGVJdq2huQNrLrDKo5Oc3TWIvguhXfdRkMCJDCt9cZlIyRKdkPoPTGDn9bCb1gA6aGUglQqHWjrULL8ieiF7H/Fyv8GSOQSh8ZoTDNQVwUQp001QozADuBZ1RfAqqrQNDWs7RCcRejU7uN3I/UtIPHXti2UNohOAvO4sPamov8CTIoaA2MtvOaYxrBrF1u7fy/5lsNFZQ0hFbRogTjvOHsFeK2a5vZFZX5v0Bv+I2/THBGGEd47eN8nx1CH2xziMdiCt3BaYI4x2YgcEx85BeXTPMM7i6Yu8etwwPF4TPYkDXaDvVqPdvIlKb/unPPUQVHk8H2P2/UCqxUKxpLfyRBZlqUCtpa/qra+XHf6iTG2uOLh82dRxqezP+QUlFOOJasKAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/c5f25201cc6e22acc07b19198bc05cd0/d0b9c/Example_Training.jpg","srcSet":"/static/c5f25201cc6e22acc07b19198bc05cd0/90ed1/Example_Training.jpg 200w,\n/static/c5f25201cc6e22acc07b19198bc05cd0/2070e/Example_Training.jpg 400w,\n/static/c5f25201cc6e22acc07b19198bc05cd0/d0b9c/Example_Training.jpg 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[]},"width":800,"height":600}}}},"fields":{"path":"/blog/logistic-regression-intro/","readingTime":{"text":"17 min read"}},"tableOfContents":{"items":[{"url":"#introduction","title":"Introduction"},{"url":"#what-is-logistic-regression","title":"What is Logistic Regression?"},{"url":"#math-representation","title":"Math Representation","items":[{"url":"#data-organization","title":"Data Organization"},{"url":"#forward-propagation","title":"Forward Propagation"}]},{"url":"#","title":"$$"},{"url":"#training","title":"Training","items":[{"url":"#cost-and-loss-functions","title":"Cost and Loss Functions"},{"url":"#gradient-descent","title":"Gradient Descent"}]},{"url":"#implementation","title":"Implementation","items":[{"url":"#model-class","title":"Model Class"},{"url":"#data-import-and-function-calls","title":"Data Import and Function Calls"},{"url":"#running-the-model","title":"Running the Model"}]},{"url":"#moving-forward","title":"Moving Forward"}]}},"site":{"siteMetadata":{"title":"Patrick Youssef"}}},"pageContext":{"post_id":"/blog/logistic-regression-intro/"}},"staticQueryHashes":[]}