{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/neural-net-gradient-descent/","result":{"data":{"mdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"templateKey\": \"blog-post\",\n  \"published\": true,\n  \"title\": \"Neural Networks From Scratch: Gradient Descent\",\n  \"slug\": \"neural-net-gradient-descent\",\n  \"date\": \"2023-03-25T00:00:00.000Z\",\n  \"featureImage\": \"images/feature.jpg\",\n  \"tags\": [\"Machine Learning\", \"Gradient Descent\"],\n  \"excerpt\": \"The first in a series where I introduce how to implement gradient descent in the context of Neural Networks.\",\n  \"pinned\": true\n};\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\nvar Collapse = makeShortcode(\"Collapse\");\nvar PostVideo = makeShortcode(\"PostVideo\");\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(SeriesIntro, {\n    mdxType: \"SeriesIntro\"\n  }), mdx(\"h1\", {\n    \"id\": \"introduction\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Introduction\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#introduction\",\n    \"aria-label\": \"introduction permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Gradient descent is a simple and intuitive optimization method that is common across a large space of problems ranging from simple linear regression to neural networks. With this, we can optimize against a given function and determine parameters that give us the best performance. By the end of this post, you should have a good idea on how this optimizer works and how to apply it to new problems.\"), mdx(\"h1\", {\n    \"id\": \"prior-knowledge\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Prior Knowledge\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#prior-knowledge\",\n    \"aria-label\": \"prior knowledge permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"There are a few concepts that you should know before coming into this post, in general I try to explain everything clearly but it helps if you have prior exposure to these concepts.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"NumPy:\"), \" We'll be using NumPy as the data storage and computation backbone. If you're not familiar with NumPy matrices and how to manipulate them take a look at \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://numpy.org/doc/stable/user/absolute_beginners.html\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"this resource\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Calculus:\"), \" We rely heavily on derivatives, in particular, \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.mathsisfun.com/calculus/derivatives-partial.html\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"partial derivatives\"), \" are important!\")), mdx(\"h1\", {\n    \"id\": \"gradients\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Gradients\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#gradients\",\n    \"aria-label\": \"gradients permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Before we step into how we use the gradient to help in our optimization problem, let's first build some understanding around the gradient. For those who are unfamiliar with partial derivatives or want a refresher you can take a look at the aside below.\"), mdx(Collapse, {\n    title: \"Partial Derivative Refresher\",\n    mdxType: \"Collapse\"\n  }, mdx(PartialDeriv, {\n    mdxType: \"PartialDeriv\"\n  })), mdx(\"h2\", {\n    \"id\": \"gradient-intuition\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Gradient Intuition\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#gradient-intuition\",\n    \"aria-label\": \"gradient intuition permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The gradient of a function $\\\\nabla f$ is actually quite a simple concept but can be so powerful when used correctly. The way to think about the gradient intuitively is that it's a vector that points in the direction of greatest increase of a certain function. \"), mdx(\"p\", null, \"It's a vector in the sense that it has the direction where max increase happens as well as the magnitude of the increase. The components of the vector lie in the space of the function so if the function takes one input the gradient is only one value and direction.\"), mdx(\"h2\", {\n    \"id\": \"gradient-definition\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Gradient Definition\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#gradient-definition\",\n    \"aria-label\": \"gradient definition permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"To be more formal, we can define the gradient in terms of partial derivatives such that we could actually compute a gradient. Consider a function $f(x, y, z)$ that depends on multiple input variables. The gradient $\\\\nabla f$ is a vector of partial derivatives.\"), mdx(\"p\", null, \"$$\\n\\\\nabla f =\\n\\\\begin{bmatrix}\\n\\\\frac{\\\\partial f}{\\\\partial x} \", \"\\\\\", \"[5pt]\", \"\\n\\\\frac{\\\\partial f}{\\\\partial y} \", \"\\\\\", \"[5pt]\", \"\\n\\\\frac{\\\\partial f}{\\\\partial z} \", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"So if you're standing at a point $(x_0, y_0, z_0)$ and compute the associated gradient at that point, you'd be getting the vector pointing in the direction from where you are such that you would climb $f$ the fastest. A clearer way to show this is that we evaluate each partial derivative at our point.\"), mdx(\"p\", null, \"$$\\n\\\\nabla f(x_0, y_0, z_0) =\\n\\\\begin{bmatrix}\\n\\\\frac{\\\\partial f}{\\\\partial x}(x_0, y_0, z_0) \", \"\\\\\", \"[5pt]\", \"\\n\\\\frac{\\\\partial f}{\\\\partial y}(x_0, y_0, z_0) \", \"\\\\\", \"[5pt]\", \"\\n\\\\frac{\\\\partial f}{\\\\partial z}(x_0, y_0, z_0) \", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"Don't be worried about the math for now, I just want you to take the intuition and roll with it for now. If you plan to implement this from scratch you'll need to actually evaluate this but for now just know it points to where we climb $f$ fastest!\"), mdx(\"h1\", {\n    \"id\": \"gradient-descent\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Gradient Descent\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#gradient-descent\",\n    \"aria-label\": \"gradient descent permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Now that we have an understanding of how to compute gradients and what information they give us, let's use it to optimize our problems through gradient descent.\"), mdx(\"h2\", {\n    \"id\": \"descent-intuition\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Descent Intuition\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#descent-intuition\",\n    \"aria-label\": \"descent intuition permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let imagine that you are stranded on a large hill and really dense fog surrounds you such that you can only see a few feet around. You want to \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"descend\"), \" to the bottom of the hill back to safety but you can't see far enough to path out how to get to the bottom.\"), mdx(\"p\", null, \"You decide that the best thing you can do is to follow the direction of greatest descent local to what you can see. As you continue to descend you get new information about your surroundings so you change your direction every so often to the new greatest descent. Eventually you'll either reach a valley in the hill landscape or the true bottom where you're trying to be.\"), mdx(\"p\", null, \"What you've just done is gradient descent down the hill! In the next section we'll associate these bits of intuition to the math and see how it works.\"), mdx(\"h2\", {\n    \"id\": \"descent-definition\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Descent Definition\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#descent-definition\",\n    \"aria-label\": \"descent definition permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's formally define gradient descent and associate our example back to the components of it. In the simplest form, gradient descent is repeatedly computing the following equation.\"), mdx(\"p\", null, \"$$\\n\\\\theta_{i+1} = \\\\theta_i - \\\\alpha \\\\nabla f(\\\\theta_i)\\n$$\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"$f$ is the function we're minimizing with gradient descent, it's the surface height of the hill\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"$\\\\theta$ is the set of variables that $f$ needs, it's the $x$ and $y$ position on the hill looking top down\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"$\\\\nabla f(\\\\theta)$ is the gradient of $f$ with respect to $\\\\theta$, it's the direction we move in $x$ and $y$ on the hill\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"$\\\\alpha$ is a scalar for the gradient to decrease how big of a step we take, often called learning rate\")), mdx(\"h2\", {\n    \"id\": \"local-vs-global-minima\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Local vs Global Minima\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#local-vs-global-minima\",\n    \"aria-label\": \"local vs global minima permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Another aspect of the problem we should try to connect is the idea of an intermediate valley in the hill landscape versus the true bottom. Formally this difference is called local and global minimas.\"), mdx(\"p\", null, \"With gradient descent there is a problem in which depending on the loss function we are not guaranteed that we'll land at the global minima which is the lowest point over the entire landscape. Let's consider some simple cases to see why this is the case.\"), mdx(\"h3\", {\n    \"id\": \"convex-function\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Convex Function\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#convex-function\",\n    \"aria-label\": \"convex function permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"In a convex function, there is only one minima and that implies it's the global minima. A simple convex function is a parabola so when we execute gradient descent over this function we always land at the global minimum no matter where we start.\"), mdx(PostVideo, {\n    video: grad,\n    mdxType: \"PostVideo\"\n  }), mdx(\"p\", null, \"In the video, the blue represents the partial derivative $\\\\partial f / \\\\partial x$ so with descent we expect to go opposite this direction as we change the value of x with the negative of the gradient.\"), mdx(\"h3\", {\n    \"id\": \"multiple-minima-function\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Multiple Minima Function\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#multiple-minima-function\",\n    \"aria-label\": \"multiple minima function permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's consider a function where there are multiple minimas, in particular we have two equivalent global minimas and one local minima in the center. When we start gradient descent at various points we see that we don't always get to the global minima.\"), mdx(PostVideo, {\n    video: grad_problem,\n    mdxType: \"PostVideo\"\n  }), mdx(\"h3\", {\n    \"id\": \"resolutions\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Resolutions\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#resolutions\",\n    \"aria-label\": \"resolutions permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"In practice, we don't even know what the loss landscape looks like for large models, but this problem exists no matter what. There is research done on variations of gradient descent to help mitigate these problems as well as improve training speed and overall performance. The go-to right now is the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://optimization.cbe.cornell.edu/index.php?title=Adam\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"Adam optimizer\"), \" which varies $\\\\alpha$ automatically based on patterns observed in prior iterations. \"), mdx(\"p\", null, \"Just to give you an idea what these more complicated landscapes look like, consider \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1712.09913\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"this work\"), \" done to try to visualize them! Mind you, in most applications the number of inputs variables is larger than we can visualize so this is an attempt at visualizing.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/2abf3ff988462db683a129bac223690d/7f391/noshort.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"77.33333333333333%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEhklEQVQ4yyWTzU8TdhyHv30DEuNbptNBX0XGJBpf8AW2QbWAUihSXixQXqSlFPpCwVoopbAWBFoRhvJSVKAIrUWcSsk2t8OyxIXExexisoOH7eBhu2yLXrYZaH+fxfn8Ac/lyUNERCGBlFZJQunpOwlUxxlNEH4zwxPhDlfye4Av+jXAEz0PcsXrQY748ZhA+LB96x6D98ghQlUZBQWS3EWBVP4lL/XgCk8moxBfSnf5MvJuE/I8W1PIvj25wL1LiO4UMWzJIphlEhhFIlj3CGHZK4RZLEJb5oG/us+dbA6+lyZb4knfRLhSBPgitsCXvKbr20QUEeyjFE8W51RDJnnTPrJ0yY9iqDgr3qPMiveV5sY953Ni7qKsmDnv+Kbt9LENlzoPraqc10M7xCtTPCGcO1I2TO8nY04g+YOmtohokS/hLZOYVjgy72SSCL3FpzeHTVpMOwxsuL0JAy1a5mupgV2rZp4LBWywRomJy7p/Go5mvK6VpKA+60CsS7oPy1zpE7qTIOU+4MroYUJq2kSS6N/e/VLMVuXFx/scmBwfwbWxcbSYbDCbrTDq9Wy4WYOwx8K+muiDuTQf5rNZzFZ0evPqThnukniebm2RcBcohZZ4kuXr+9PQrzy5ETKV436/lbk9ftbWG4Cq1gV1XQeMVgeGPD0sMDqEOZ+L6S6UsVZDPRzZuRshSkY4MbX7/yhLiTJFmCPB/O7U+KROxUa7rWzQaYN/dAp2XxRnq4dQrh+EpskNnaWPmS974fO4mNP1GdM02lhpxoexwcS9uM9N1dIiV0RLfOl6hCfDWOIHsQlNPkLz06zXPwvvjUfwzqyjzh6CumkM9fZFFNb6UKLtQv/obVyZfIDCMjOrOJwGjSQFuqTd8rcyZ5gnxc0EMa6kp8ZnSrMRvjHMFtZ+xLXQT3BPPUPrwBPUd66i6OJNnFL6kKcZRHmTH1rTCDRVOlYpP4mqzIy/C6V702lJIHVFSHrPv1v823ThCaxay+MP/HYsRFbZyPxTdEw8Z/V9P7B8bZAdONGLTMUoq7BEoWwMIq+oBvWNzfHiIhUqz+b8WXlItp1WSEjTu2TkOrr/42C1At8Pm7EeGWd3F2bZjQp1PNxiidkavDG56iqTl82hrPUxU+qiqDCtQKG+xM6UGOJ5JQ3QfVrwAkRcCiXt44ZO5SRMGdU0Z6rwrY05sR4NvulsqkVDfg4ClWfwtU2NR1PXmMH7hNU6vkPdpSizdH3OjO1XoGro2jQeUcCffOjpz28/BkCz/R304ouL9PAYcQJjw8/azCYoC0s2FWr9S1Vpw1qkW78WcJtRa52MWTwr8e6hOdbhucW0bddjNU09mxXZx3FOUdx54dhBosnyXLptraIem5Xb0emixja3uEBVXZ973ng4W27YudJbTs0V1eTscn5b3dwHfecsbAPLsHrvwdizBIOlB6VK5YI+J41eHad3hF16GnA6aLDrEsfo8pNCWUmfFOnpiMJMI/YWvsY6QFUtnoxqQ4/non3ml/ahKHNcjb6yD9x5qbf1t7+z8OlMqZ7zH9hxIQZNqgB3AAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Neural Net Loss Landscape\",\n    \"title\": \"Neural Net Loss Landscape\",\n    \"src\": \"/static/2abf3ff988462db683a129bac223690d/8537d/noshort.png\",\n    \"srcSet\": [\"/static/2abf3ff988462db683a129bac223690d/eed55/noshort.png 300w\", \"/static/2abf3ff988462db683a129bac223690d/7491f/noshort.png 600w\", \"/static/2abf3ff988462db683a129bac223690d/8537d/noshort.png 1200w\", \"/static/2abf3ff988462db683a129bac223690d/d2cc9/noshort.png 1800w\", \"/static/2abf3ff988462db683a129bac223690d/7f391/noshort.png 1811w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Neural Net Loss Landscape\"), \"\\n  \")), mdx(\"h1\", {\n    \"id\": \"hill-descent\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Hill Descent\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#hill-descent\",\n    \"aria-label\": \"hill descent permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Before we step into linear regression, let's try to emulate the hill descent.\"), mdx(\"h2\", {\n    \"id\": \"surface-and-gradient-equation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Surface and Gradient Equation\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#surface-and-gradient-equation\",\n    \"aria-label\": \"surface and gradient equation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"For our primary function $f$, we need to model the height of where we would be on the hill at a position $(x,y)$ which simply means we need something of the form $f(x,y)$. For this example I opted to use simple sin waves over the multiple variables.\"), mdx(\"p\", null, \"$$\\nf(x,y) = \\\\sin(x) \\\\sin(y)\\n$$\"), mdx(\"p\", null, \"Why is height the right equation to minimize? Well consider that whatever function we model we will end up minimizing its value so we want that behavior to match our goals. In this case, we are trying to get to the base of the hill which means we want to minimize our height. Let's go ahead and compute the corresponding partial derivatives. \"), mdx(\"p\", null, \"$$\\n\\\\frac{\\\\partial f}{\\\\partial x} = \\\\cos(x) \\\\sin(y)\\n$$\\n$$\\n\\\\frac{\\\\partial f}{\\\\partial y} = \\\\sin(x) \\\\cos(y)\\n$$\"), mdx(\"p\", null, \"$$\\n\\\\nabla f(x, y) =\\n\\\\begin{bmatrix}\\n\\\\frac{\\\\partial f}{\\\\partial x} \", \"\\\\\", \"[5pt]\", \"\\n\\\\frac{\\\\partial f}{\\\\partial y}\\n\\\\end{bmatrix} =\\n\\\\begin{bmatrix}\\n\\\\cos(x) \\\\sin(y) \", \"\\\\\", \"\\n\\\\sin(x) \\\\cos(y)\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"What we have now are the equations to know how high we are based on our position and the associated gradient for those positions to know which direction we should follow!\"), mdx(\"h2\", {\n    \"id\": \"visualizing-the-descent\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Visualizing the Descent\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#visualizing-the-descent\",\n    \"aria-label\": \"visualizing the descent permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's try to visualize what is going on and see if we can draw some conclusions to help our understanding. First, let's initialize some points randomly on these hills and execute gradient descent on each of them to see where they end up.\"), mdx(\"h3\", {\n    \"id\": \"descent-animation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Descent Animation\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#descent-animation\",\n    \"aria-label\": \"descent animation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's go ahead and take a look at the hills example realized! What you can see is a 3D view and a top down view of the paths taken by each of the points.\"), mdx(PostVideo, {\n    video: hill_descent,\n    mdxType: \"PostVideo\"\n  }), mdx(\"p\", null, \" As we can see they all reach minimums which is what we expect! Something to note is that you can see that depending on where the points start they have a destined minimum they'll end up in, in other landscapes this may mean that a point will terminate at a local not a global minima!\"), mdx(\"h3\", {\n    \"id\": \"gradient-field\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Gradient Field\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#gradient-field\",\n    \"aria-label\": \"gradient field permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"We can evaluate the gradient at a grid of points across the space we're optimizing in. We know in this case that the gradient produces a vector over $x$ and $y$ so we can plot the gradient as an arrow pointing in the gradient direction. This grid of arrows is called a \\\"gradient vector field\\\".\"), mdx(\"p\", null, \"Plotting the paths from before over the vector field we can see that the path the points take almost exactly follows the negative of the gradient which makes sense! The gradient points in the direction of greatest \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"increase\"), \" so when we descend we follow the negative of the gradient to follow the path of greatest \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"decrease\"), \". We could just as easily follow the gradient if we wanted to maximize a function called gradient ascent but this isn't nearly as common.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/785bbc1e37a33dbe402e89837defe16a/5c188/grad_path.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAMBBAX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB1kvWtokT/8QAGxAAAQQDAAAAAAAAAAAAAAAAAQADEjECEyL/2gAIAQEAAQUCdku5inWzmtRkK//EABYRAAMAAAAAAAAAAAAAAAAAABAhMf/aAAgBAwEBPwGMf//EABcRAAMBAAAAAAAAAAAAAAAAAAEQITH/2gAIAQIBAT8B2Ff/xAAaEAACAgMAAAAAAAAAAAAAAAAAARARISJR/9oACAEBAAY/AtRcjAncf//EABsQAAICAwEAAAAAAAAAAAAAAAABESExQVFh/9oACAEBAAE/IXSoWPY9GMYKUQPjWhWrZP/aAAwDAQACAAMAAAAQXw//xAAYEQADAQEAAAAAAAAAAAAAAAAAAREhQf/aAAgBAwEBPxBOY1vhT//EABcRAAMBAAAAAAAAAAAAAAAAAAERQRD/2gAIAQIBAT8QCkVz/8QAHBABAAMAAgMAAAAAAAAAAAAAAQARMSGxQVFx/9oACAEBAAE/EFjlClfHyLgGoUuGgtd7iap9HYoFAA22ASuu5//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Gradient Field with Paths\",\n    \"title\": \"Gradient Field with Paths\",\n    \"src\": \"/static/785bbc1e37a33dbe402e89837defe16a/6c738/grad_path.jpg\",\n    \"srcSet\": [\"/static/785bbc1e37a33dbe402e89837defe16a/73b64/grad_path.jpg 300w\", \"/static/785bbc1e37a33dbe402e89837defe16a/3ad8d/grad_path.jpg 600w\", \"/static/785bbc1e37a33dbe402e89837defe16a/6c738/grad_path.jpg 1200w\", \"/static/785bbc1e37a33dbe402e89837defe16a/8b34c/grad_path.jpg 1800w\", \"/static/785bbc1e37a33dbe402e89837defe16a/111a0/grad_path.jpg 2400w\", \"/static/785bbc1e37a33dbe402e89837defe16a/5c188/grad_path.jpg 2560w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Gradient Field with Paths\"), \"\\n  \")), mdx(\"h1\", {\n    \"id\": \"linear-regression\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Linear Regression\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#linear-regression\",\n    \"aria-label\": \"linear regression permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Now that we have the foundation to apply this method, let's implement gradient descent for linear regression and analyze some of the behavior. In this section I focus more on the conceptual side of this problem and not the code but you can see how I implemented it in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/blog/neural-net-gradient-descent/#implementation\"\n  }, \"next section\"), \".\"), mdx(\"h2\", {\n    \"id\": \"problem-statement\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Problem Statement\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#problem-statement\",\n    \"aria-label\": \"problem statement permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The linear regression problem is to determine the line that best fits a set of data. To be precise, let's define some of the components of this.\"), mdx(\"h3\", {\n    \"id\": \"dataset\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Dataset\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#dataset\",\n    \"aria-label\": \"dataset permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"We need the inputs $X$ and the corresponding results $y$. $X$ contains all of the samples in our dataset. An entry $X_i$ as a particular sample out of our whole dataset with it's corresponding $y_i$. The number of samples in our dataset is denoted as $N$.\"), mdx(\"h3\", {\n    \"id\": \"model\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Model\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#model\",\n    \"aria-label\": \"model permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The general model, as expected, is a simple linear model that takes in the input variables $X$ and computes a prediction $\\\\hat{y}$. This can be thought of as the following equation.\"), mdx(\"p\", null, \"$$\\n\\\\hat{y} = w X + b\\n$$\"), mdx(\"p\", null, \"Where the weight $w$ is the slope of the line and the bias $b$ is the line vertical offset. We are considering the scalar case here where $X$ is a single number for each example in our dataset but we could also have the case where $X$ is a vector of multiple inputs for each sample and as such $w$ would also be a vector. In the next post, we'll look more into what that looks like but I want start simple.\"), mdx(\"p\", null, \"Often it will help to think about this model working on a single sample at a time which merely implies that we index a sample out of $X$ and that only results in the prediction for that sample.\"), mdx(\"p\", null, \"$$\\n\\\\hat{y_i} = w X_i + b\\n$$\"), mdx(\"h2\", {\n    \"id\": \"function-to-minimize\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Function to Minimize\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#function-to-minimize\",\n    \"aria-label\": \"function to minimize permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"In this problem, we want to formulate a function that by minimizing we'll achieve the expected behavior. One way to think about this is we want a function that indicates how bad we are doing and we want to minimize this \\\"badness\\\". This is commonly referred to as a \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Loss_function#:~:text=In%20mathematical%20optimization%20and%20decision,cost%22%20associated%20with%20the%20event.\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"loss function\"), \".\"), mdx(\"p\", null, \"In the case of linear regression, we're looking to produce a real-valued number and compare it to the dataset samples. The loss function used for this is called the mean-squared error loss (MSE).\"), mdx(\"p\", null, \"$$\\n\\\\text{MSE} = \\\\frac{1}{N} \\\\sum^{n}_{i=1} (y_i - \\\\hat{y_i})^2\\n$$\"), mdx(\"p\", null, \"To be more inline with our prior labeling we'll call MSE our loss $L$. We can sub in our prior model for prediction for $\\\\hat{y_i}$ to get the final loss description in terms of our model parameters $w$ and $b$. This is because we can think of $w$ and $b$ as our knobs that we can tweak to change where the line is!\"), mdx(\"p\", null, \"$$\\nL = \\\\frac{1}{N} \\\\sum^{n}_{i=1} (y_i - (wx_i + b))^2\\n$$\"), mdx(\"p\", null, \"As we are trying to minimize the loss using gradient descent, we need to compute the needed gradients for our model parameters $w$ and $b$ which are the following. If you want to see step by step how I computed these partials look at the aside below.\"), mdx(\"p\", null, \"$$\\n\\\\frac{\\\\partial L}{\\\\partial w} = \\\\frac{-2}{N} \\\\sum^{n}_{i=1} x_i (y_i - (wx_i + b))\\n$$\"), mdx(\"p\", null, \"$$\\n\\\\frac{\\\\partial L}{\\\\partial b} = \\\\frac{-2}{N} \\\\sum^{n}_{i=1} (y_i - (wx_i + b))\\n$$\"), mdx(Collapse, {\n    title: \"Derivation of Gradients\",\n    mdxType: \"Collapse\"\n  }, mdx(LinearRegressionGrad, {\n    mdxType: \"LinearRegressionGrad\"\n  })), mdx(\"p\", null, \"With these gradients, we can execute gradient descent by updating our model parameters by their associated gradients evaluated at each iteration.\"), mdx(\"h2\", {\n    \"id\": \"linear-dataset-example\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Linear Dataset Example\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#linear-dataset-example\",\n    \"aria-label\": \"linear dataset example permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's actually step into an example where we can see how the model trains and eventually performs. We'll start with modeling a dataset where the underlying trend of the data is linear so we would expect this model to perform well.\"), mdx(\"h2\", {\n    \"id\": \"linear-dataset\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Linear Dataset\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#linear-dataset\",\n    \"aria-label\": \"linear dataset permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The construction of out linear dataset is very simple! We start with a line with a known $w$ and $b$, in this case $w=2$ and $b=1$, and we apply noise to the points just as we would expect real data to have. The noise is merely sampling from a normal distribution $\\\\mathcal{N}(0, \\\\sigma^2)$. In a real example, we would not know what the true line is and we use linear regression to estimate $w$ and $b$ of that line.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/f950012281bfaf5a1db7d326cf2bbabd/5c188/linear_data.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3opQf//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEAAQUCX//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABoQAQADAAMAAAAAAAAAAAAAAAEAIUEQMWH/2gAIAQEAAT8h2LhbDq4ivkKzj//aAAwDAQACAAMAAAAQ8M//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAADAAIDAQAAAAAAAAAAAAAAAREhMUFRYXH/2gAIAQEAAT8QjbOudGXL9NF+hIWcptiRFCFo/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"True Line and Noisy Points\",\n    \"title\": \"True Line and Noisy Points\",\n    \"src\": \"/static/f950012281bfaf5a1db7d326cf2bbabd/6c738/linear_data.jpg\",\n    \"srcSet\": [\"/static/f950012281bfaf5a1db7d326cf2bbabd/73b64/linear_data.jpg 300w\", \"/static/f950012281bfaf5a1db7d326cf2bbabd/3ad8d/linear_data.jpg 600w\", \"/static/f950012281bfaf5a1db7d326cf2bbabd/6c738/linear_data.jpg 1200w\", \"/static/f950012281bfaf5a1db7d326cf2bbabd/8b34c/linear_data.jpg 1800w\", \"/static/f950012281bfaf5a1db7d326cf2bbabd/111a0/linear_data.jpg 2400w\", \"/static/f950012281bfaf5a1db7d326cf2bbabd/5c188/linear_data.jpg 2560w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"True Line and Noisy Points\"), \"\\n  \")), mdx(\"p\", null, \"We can now execute a linear regression to solve for $w$ and $b$ and see how well we match our known true line based on fitting to the samples alone.\"), mdx(\"h2\", {\n    \"id\": \"training-visualization\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Training Visualization\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#training-visualization\",\n    \"aria-label\": \"training visualization permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"What you can see in the video below is the loss landscape on the left and the history of our $(w,b)$ values as the point traveling through it and on the right you can see the predicted line in red plotting the corresponding line for that $(w,b)$ based on the position in the descent. We can see the loss value is quite low and the fit seems good!\"), mdx(PostVideo, {\n    video: loss_travel,\n    mdxType: \"PostVideo\"\n  }), mdx(\"p\", null, \"One way to think about this is that the position of the point $(w,b)$ defines a line that will have a corresponding height that is the current loss. By traversing down the loss landscape we find the $(w,b)$ such that the corresponding loss in minimal. \"), mdx(\"h2\", {\n    \"id\": \"parabolic-dataset-example\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Parabolic Dataset Example\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#parabolic-dataset-example\",\n    \"aria-label\": \"parabolic dataset example permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's consider a different example where the underlying trend of the data isn't a line anymore but instead a parabola. \"), mdx(\"h2\", {\n    \"id\": \"parabolic-dataset\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Parabolic Dataset\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#parabolic-dataset\",\n    \"aria-label\": \"parabolic dataset permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"In the same way that we created the linear data, we will sample points along the function and apply noise to each of the points from a normal distribution.\"), mdx(\"h2\", {\n    \"id\": \"training-visualization-1\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Training Visualization\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#training-visualization-1\",\n    \"aria-label\": \"training visualization 1 permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The visualization here is similar to the prior but just with the parabolic data. As we can see the method is working quite well as we've reached the minimum loss again.\"), mdx(PostVideo, {\n    video: loss_travel_parabola,\n    mdxType: \"PostVideo\"\n  }), mdx(\"h2\", {\n    \"id\": \"comparing-losses\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Comparing Losses\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#comparing-losses\",\n    \"aria-label\": \"comparing losses permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"You may have noticed that both examples reach the minimum which would indicate that we should have reached a point where the model performs well, right? \"), mdx(\"p\", null, \"Not quite, in fact there is a whole concept in machine learning called the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"bias/variance tradeoff\"), \". The general idea of this is that there are two extremes that result in poor performance.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"High Bias\"), \": The model is too \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"inflexible\"), \" to model the data correctly\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Happens when models are too simple for patterns in the data\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This is where we are, there is no world where a line will correctly fit a parabola\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"High Variance\"), \": The model is too \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"flexible\"), \" to generalize the data correctly\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Happens when models are too complex are starts fitting to patterns that aren't \\\"real\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Large models can have an issue where they fit to noise and don't generalize the data well\")))), mdx(\"p\", null, \"Somewhere in the middle there is a goldilocks region where the model is flexible enough to encompass the complexity of the data but not so flexible that it starts \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/Overfitting\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"overfitting\"), \".\"), mdx(\"p\", null, \"So when we compare slices along the minimums of the loss landscapes we see that sure we may have reached the minimum but that minimum is very high for the parabolic case! \"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/b4e4849df6cedba1275d587def754aba/5c188/loss_compare.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3rBQf//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEAAQUCX//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABcQAAMBAAAAAAAAAAAAAAAAAAAQgUH/2gAIAQEAAT8hr0i//9oADAMBAAIAAwAAABAjz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABoQAQADAQEBAAAAAAAAAAAAAAEAETEhcWH/2gAIAQEAAT8QbVRUOR9YZKbca9heW59hk//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Comparison of Losses\",\n    \"title\": \"Comparison of Losses\",\n    \"src\": \"/static/b4e4849df6cedba1275d587def754aba/6c738/loss_compare.jpg\",\n    \"srcSet\": [\"/static/b4e4849df6cedba1275d587def754aba/73b64/loss_compare.jpg 300w\", \"/static/b4e4849df6cedba1275d587def754aba/3ad8d/loss_compare.jpg 600w\", \"/static/b4e4849df6cedba1275d587def754aba/6c738/loss_compare.jpg 1200w\", \"/static/b4e4849df6cedba1275d587def754aba/8b34c/loss_compare.jpg 1800w\", \"/static/b4e4849df6cedba1275d587def754aba/111a0/loss_compare.jpg 2400w\", \"/static/b4e4849df6cedba1275d587def754aba/5c188/loss_compare.jpg 2560w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Comparison of Losses\"), \"\\n  \")), mdx(\"h1\", {\n    \"id\": \"learning-rate-tuning\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Learning Rate Tuning\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#learning-rate-tuning\",\n    \"aria-label\": \"learning rate tuning permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"The learning rate $\\\\alpha$ is generally regarded as how big of a step to take in the direction of the gradient during descent. Changing the value of the learning rate has massive implications on the descent time and results. Let's consider some example and try to understand how to pick an adequate value for the learning rate.\"), mdx(\"h2\", {\n    \"id\": \"general-behaviors\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"General Behaviors\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#general-behaviors\",\n    \"aria-label\": \"general behaviors permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"h2\", {\n    \"id\": \"comparing-learning-rates\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Comparing Learning Rates\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#comparing-learning-rates\",\n    \"aria-label\": \"comparing learning rates permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Let's consider the convex function below where we know the optimal value lies at $(0,0)$. With different learning rates we will see how the behavior and results change!\"), mdx(\"p\", null, \"$$\\nf(x,y) = \\\\frac{5 x ^ 2 + y ^ 2}{2}\\n$$\"), mdx(\"p\", null, \"$$\\n\\\\nabla f =\\n\\\\begin{bmatrix}\\n\\\\frac{\\\\partial f}{\\\\partial x} \", \"\\\\\", \"[5pt]\", \"\\n\\\\frac{\\\\partial f}{\\\\partial y} \", \"\\\\\", \"\\n\\\\end{bmatrix} =\\n\\\\begin{bmatrix}\\n5x \", \"\\\\\", \"[5pt]\", \"\\ny \", \"\\\\\", \"\\n\\\\end{bmatrix}\\n$$\"), mdx(\"p\", null, \"Across the following examples, the number of iterations is kept the same as to easily compare their behaviors but you could always adjust this when solving a problem.\"), mdx(\"h3\", {\n    \"id\": \"small-learning-rate\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Small Learning Rate\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#small-learning-rate\",\n    \"aria-label\": \"small learning rate permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"When the learning rate $\\\\alpha$ is too small there isn't a concern of instability but getting to a minima takes forever! This is not a big deal for small problems but on large problems where the computation is not trivial wasting time taking micro steps is not recommended. \"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/921b460c0396ef137e1b08f5cd1e8d16/9e016/learningrate_0.01.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIEBf/EABQBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdC3MgD/AP/EABoQAQABBQAAAAAAAAAAAAAAAAECAAMQETH/2gAIAQEAAQUCvyRjytGf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAEEH/2gAIAQIBAT8Bh//EABcQAAMBAAAAAAAAAAAAAAAAAAERIFH/2gAIAQEABj8CC2v/xAAZEAEAAwEBAAAAAAAAAAAAAAABABAhMVH/2gAIAQEAAT8hCFG/dKdCJlf/2gAMAwEAAgADAAAAEMzP/8QAFhEAAwAAAAAAAAAAAAAAAAAAARBB/9oACAEDAQE/EKV//8QAFhEBAQEAAAAAAAAAAAAAAAAAARAR/9oACAECAQE/EF0E/8QAGxABAQEAAwEBAAAAAAAAAAAAAREAIVFxMUH/2gAIAQEAAT8QRkH1PDUC9rfdxeLlKpv6YBQDzQ63/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Slow and Steady\",\n    \"title\": \"Slow and Steady\",\n    \"src\": \"/static/921b460c0396ef137e1b08f5cd1e8d16/6c738/learningrate_0.01.jpg\",\n    \"srcSet\": [\"/static/921b460c0396ef137e1b08f5cd1e8d16/73b64/learningrate_0.01.jpg 300w\", \"/static/921b460c0396ef137e1b08f5cd1e8d16/3ad8d/learningrate_0.01.jpg 600w\", \"/static/921b460c0396ef137e1b08f5cd1e8d16/6c738/learningrate_0.01.jpg 1200w\", \"/static/921b460c0396ef137e1b08f5cd1e8d16/8b34c/learningrate_0.01.jpg 1800w\", \"/static/921b460c0396ef137e1b08f5cd1e8d16/111a0/learningrate_0.01.jpg 2400w\", \"/static/921b460c0396ef137e1b08f5cd1e8d16/9e016/learningrate_0.01.jpg 3200w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Slow and Steady\"), \"\\n  \")), mdx(\"h3\", {\n    \"id\": \"good-learning-rate\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Good Learning Rate\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#good-learning-rate\",\n    \"aria-label\": \"good learning rate permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"When the learning rate $\\\\alpha$ is well chosen we swiftly converge to the minima without any oscillations or sporadic behavior. We reach the optimal point on our loss surface within ~25 iterations and hold there.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/195408a7f8cbe3e28520d352c5d48bf7/9e016/learningrate_0.1.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIEBf/EABQBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdC3MgD/AP/EABoQAQABBQAAAAAAAAAAAAAAAAECAAMQETH/2gAIAQEAAQUCvyRjytGf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAEEH/2gAIAQIBAT8Bh//EABcQAAMBAAAAAAAAAAAAAAAAAAERIFH/2gAIAQEABj8CC2v/xAAZEAEAAwEBAAAAAAAAAAAAAAABABAhMRH/2gAIAQEAAT8hCFG9Up0Ih5yv/9oADAMBAAIAAwAAABDPz//EABYRAAMAAAAAAAAAAAAAAAAAAAEQQf/aAAgBAwEBPxClf//EABYRAQEBAAAAAAAAAAAAAAAAAAEQEf/aAAgBAgEBPxBdBP/EABsQAQEBAAIDAAAAAAAAAAAAAAERADFxIUFR/9oACAEBAAE/EEZBynRlGrbz3kB8ZSqb7MBAJ1gPhv/Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Adequately Fast\",\n    \"title\": \"Adequately Fast\",\n    \"src\": \"/static/195408a7f8cbe3e28520d352c5d48bf7/6c738/learningrate_0.1.jpg\",\n    \"srcSet\": [\"/static/195408a7f8cbe3e28520d352c5d48bf7/73b64/learningrate_0.1.jpg 300w\", \"/static/195408a7f8cbe3e28520d352c5d48bf7/3ad8d/learningrate_0.1.jpg 600w\", \"/static/195408a7f8cbe3e28520d352c5d48bf7/6c738/learningrate_0.1.jpg 1200w\", \"/static/195408a7f8cbe3e28520d352c5d48bf7/8b34c/learningrate_0.1.jpg 1800w\", \"/static/195408a7f8cbe3e28520d352c5d48bf7/111a0/learningrate_0.1.jpg 2400w\", \"/static/195408a7f8cbe3e28520d352c5d48bf7/9e016/learningrate_0.1.jpg 3200w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Adequately Fast\"), \"\\n  \")), mdx(\"h3\", {\n    \"id\": \"large-learning-rate\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Large Learning Rate\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#large-learning-rate\",\n    \"aria-label\": \"large learning rate permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"When the learning rate $\\\\alpha$ is a bit large we have some oscillations as the descent settles into the final path. Looking just at the loss curve everything seems perfectly fine and in practice this is fine! One issue is sometimes you'll see a rapid decay of loss at first and then unstable behavior as you settle in closer.\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/361ab0a97111533dd9988f8db03e9beb/9e016/learningrate_0.25.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIEBf/EABQBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdC3MgD/AP/EABoQAQABBQAAAAAAAAAAAAAAAAECAAMQETH/2gAIAQEAAQUCvyRjytGf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFhEAAwAAAAAAAAAAAAAAAAAAARAx/9oACAECAQE/ATF//8QAFxAAAwEAAAAAAAAAAAAAAAAAAREgUf/aAAgBAQAGPwILa//EABkQAQACAwAAAAAAAAAAAAAAAAEAEBEhMf/aAAgBAQABPyEIUb7Up0Jgxyv/2gAMAwEAAgADAAAAEM/P/8QAFhEAAwAAAAAAAAAAAAAAAAAAARBB/9oACAEDAQE/EKV//8QAFhEBAQEAAAAAAAAAAAAAAAAAARAR/9oACAECAQE/EFoJ/8QAHBABAAICAwEAAAAAAAAAAAAAAQARMXEhQVGB/9oACAEBAAE/EEZBlWiKm15zuIDxFLU32RBUD5APCf/Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"A Bit Unstable\",\n    \"title\": \"A Bit Unstable\",\n    \"src\": \"/static/361ab0a97111533dd9988f8db03e9beb/6c738/learningrate_0.25.jpg\",\n    \"srcSet\": [\"/static/361ab0a97111533dd9988f8db03e9beb/73b64/learningrate_0.25.jpg 300w\", \"/static/361ab0a97111533dd9988f8db03e9beb/3ad8d/learningrate_0.25.jpg 600w\", \"/static/361ab0a97111533dd9988f8db03e9beb/6c738/learningrate_0.25.jpg 1200w\", \"/static/361ab0a97111533dd9988f8db03e9beb/8b34c/learningrate_0.25.jpg 1800w\", \"/static/361ab0a97111533dd9988f8db03e9beb/111a0/learningrate_0.25.jpg 2400w\", \"/static/361ab0a97111533dd9988f8db03e9beb/9e016/learningrate_0.25.jpg 3200w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"A Bit Unstable\"), \"\\n  \")), mdx(\"h3\", {\n    \"id\": \"unstable-learning-rate\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Unstable Learning Rate\", mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#unstable-learning-rate\",\n    \"aria-label\": \"unstable learning rate permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"When the learning rate $\\\\alpha$ is way too large we have unmanageable oscillations and in fact instead of converging on the optimal it diverges out to a massive loss (loss scale is 10^18)!\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/8472bfc7906301cc7bb64a353d23b66a/9e016/learningrate_0.5.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECBAX/xAAUAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHQtzEQwf/EABoQAQABBQAAAAAAAAAAAAAAAAECAAMQETH/2gAIAQEAAQUCvyRhytGf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFREBAQAAAAAAAAAAAAAAAAAAEEH/2gAIAQIBAT8Bh//EABcQAQADAAAAAAAAAAAAAAAAABAhMVH/2gAIAQEABj8CjWn/xAAZEAACAwEAAAAAAAAAAAAAAAAAAREhMRD/2gAIAQEAAT8hRUFrj0bNQhRnP//aAAwDAQACAAMAAAAQT8//xAAWEQADAAAAAAAAAAAAAAAAAAABEEH/2gAIAQMBAT8QpX//xAAWEQEBAQAAAAAAAAAAAAAAAAABEBH/2gAIAQIBAT8QXQT/xAAbEAEAAgMBAQAAAAAAAAAAAAABABEhMUFhgf/aAAgBAQABPxC/UOh8Jhu+dt9hdGopaM3kiDoHyUHCf//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Unusable\",\n    \"title\": \"Unusable\",\n    \"src\": \"/static/8472bfc7906301cc7bb64a353d23b66a/6c738/learningrate_0.5.jpg\",\n    \"srcSet\": [\"/static/8472bfc7906301cc7bb64a353d23b66a/73b64/learningrate_0.5.jpg 300w\", \"/static/8472bfc7906301cc7bb64a353d23b66a/3ad8d/learningrate_0.5.jpg 600w\", \"/static/8472bfc7906301cc7bb64a353d23b66a/6c738/learningrate_0.5.jpg 1200w\", \"/static/8472bfc7906301cc7bb64a353d23b66a/8b34c/learningrate_0.5.jpg 1800w\", \"/static/8472bfc7906301cc7bb64a353d23b66a/111a0/learningrate_0.5.jpg 2400w\", \"/static/8472bfc7906301cc7bb64a353d23b66a/9e016/learningrate_0.5.jpg 3200w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Unusable\"), \"\\n  \")), mdx(\"h2\", {\n    \"id\": \"where-to-start\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Where to Start\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#where-to-start\",\n    \"aria-label\": \"where to start permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"So how do you pick the right learning rate? In practice this is harder said then done and is a bit of an art. I would advise looking at similar models trained and what people used as a baseline but have fun experimenting with it too. Keep the behaviors in mind and change as you see necessary. \"), mdx(\"p\", null, \"With better optimizers like aforementioned \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://optimization.cbe.cornell.edu/index.php?title=Adam\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"Adam optimizer\"), \" much of these issues are handled automatically but it still helps to know general behaviors in plain gradient descent.\"), mdx(\"h1\", {\n    \"id\": \"extending-to-more-variables\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Extending to More Variables\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#extending-to-more-variables\",\n    \"aria-label\": \"extending to more variables permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As I alluded to before we can scale this model to work over many input variables where the input $X$ does not just have a scalar for each entry but instead of vector of multiple features that describe that sample. In the next post, we build off this model and extend this into many variables and show how to execute this efficiently.\"), mdx(\"h1\", {\n    \"id\": \"implementation\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Implementation\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#implementation\",\n    \"aria-label\": \"implementation permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"If you want to replicate this, I break down some of the code I used for linear regression here. I really wanted to focus on the conceptual understanding in this post so I pushed this to the end to focus on the process more than the code.\"), mdx(\"h2\", {\n    \"id\": \"necessary-imports\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Necessary Imports\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#necessary-imports\",\n    \"aria-label\": \"necessary imports permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Just importing the libraries that we intend to use for this example.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"import numpy as np\\nimport matplotlib.pyplot as plt\\n\")), mdx(\"h2\", {\n    \"id\": \"creating-data\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Creating Data\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#creating-data\",\n    \"aria-label\": \"creating data permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Here we create the data based on a line and add noise, if you wanted to try a different function you could add another function to represent your model but make sure to keep \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"line_func\"), \" for linear regression predictions!\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"# We'll use this function to generate predictions\\ndef line_func(x, w, b):\\n    return w * x + b\\n\\nn = 100                          # Number of points\\nX = np.linspace(-2, 2, n)        # Many points between [-2,2]\\nY = line_func(X, 2, 1)           # Generate baseline Ys\\nN = np.random.normal(0, 0.5, n)  # Noise to be added\\nS = Y + N                        # Add noise to create samples\\n\\n# Plot Data\\nplt.plot(X, Y, label=\\\"True\\\", color='black')\\nplt.scatter(X, S, label=\\\"Samples\\\", alpha=0.5)\\nplt.xlabel(\\\"x\\\")\\nplt.ylabel(\\\"y\\\")\\nplt.legend()\\nplt.grid()\\nplt.show()\\n\")), mdx(\"h2\", {\n    \"id\": \"loss-and-gradient\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Loss and Gradient\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#loss-and-gradient\",\n    \"aria-label\": \"loss and gradient permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Here we define the function to compute our mean-squared error loss as well as getting the gradients from the input data $X$, the true results $y$ and the predictions $\\\\hat{y}$.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"def MSE(Y, Yhat):\\n    return np.sum(np.square(Y-Yhat)) / Y.size\\n\\ndef get_gradient(X, Y, Yhat):\\n    # Get N\\n    N = X.size\\n\\n    # Reused so calculating once\\n    error = Y - Yhat\\n    \\n    # Get gradients\\n    dm = -2 * np.sum(X * error) / N\\n    db = -2 * np.sum(error) / N\\n    return dm, db\\n\")), mdx(\"h2\", {\n    \"id\": \"example-gradient-descent\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Example Gradient Descent\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#example-gradient-descent\",\n    \"aria-label\": \"example gradient descent permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"Here we just implement the iterative gradient descent that we've been talking about and print out the loss so we can see if it all works. At this point if you run the code you would expect something like the output on the code block.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python{out={11}}\"\n  }, \"iterations = 100 # Total number of iterations\\nalpha = 0.015    # Alpha, often called learning rate\\nmodel = [np.random.uniform(), np.random.uniform()]  # Starting random w and b\\nN = X.size # Number of samples\\n\\nfor i in range(iterations):\\n    \\n    # Predict with current model params\\n    Yhat = line_func(X, *model)\\n    \\n    # Compute Loss\\n    loss = MSE(S, Yhat)\\n    if i % 10 == 0: # Every 10 iterations print\\n        print(f\\\"Loss at iteration {i:3}: {loss:0.3f}\\\")\\n    \\n    # Get Gradient\\n    dm, db = get_gradient(X, S, Yhat)\\n    \\n    # Update w and b\\n    model[0] -= alpha * dm\\n    model[1] -= alpha * db\\n\\nLoss at iteration   0: 2.997\\nLoss at iteration  10: 1.483\\nLoss at iteration  20: 0.824\\nLoss at iteration  30: 0.537\\nLoss at iteration  40: 0.412\\nLoss at iteration  50: 0.358\\nLoss at iteration  60: 0.334\\nLoss at iteration  70: 0.323\\nLoss at iteration  80: 0.319\\nLoss at iteration  90: 0.317\\n\")), mdx(\"h2\", {\n    \"id\": \"plotting-results\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Plotting Results\", mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#plotting-results\",\n    \"aria-label\": \"plotting results permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As a check we can plot the results and see what we get! \"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\",\n    \"className\": \"language-python\"\n  }, \"plt.plot(X, Y, label=\\\"True\\\", color='black', linestyle='dashed')\\nplt.scatter(X, S, label=\\\"Samples\\\", alpha=0.5)\\nplt.plot(X, line_func(X, *model), label=\\\"Predicted Line\\\", color='red')\\nplt.xlabel(\\\"x\\\")\\nplt.ylabel(\\\"y\\\")\\nplt.legend()\\nplt.grid()\\nplt.show()\\n\")), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1200px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/8211962d4c48829f81a015d71a35024e/5c188/test_result.jpg\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3opQf//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEAAQUCX//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABoQAQACAwEAAAAAAAAAAAAAAAEAIRExQWH/2gAIAQEAAT8hvO4vC2GriOfIVyE//9oADAMBAAIAAwAAABBwz//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQACAwEBAAAAAAAAAAAAAAEAESExcVFh/9oACAEBAAE/EEVjDyZaXprsvstjFs+NswUIDk0n/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Example Result Plot\",\n    \"title\": \"Example Result Plot\",\n    \"src\": \"/static/8211962d4c48829f81a015d71a35024e/6c738/test_result.jpg\",\n    \"srcSet\": [\"/static/8211962d4c48829f81a015d71a35024e/73b64/test_result.jpg 300w\", \"/static/8211962d4c48829f81a015d71a35024e/3ad8d/test_result.jpg 600w\", \"/static/8211962d4c48829f81a015d71a35024e/6c738/test_result.jpg 1200w\", \"/static/8211962d4c48829f81a015d71a35024e/8b34c/test_result.jpg 1800w\", \"/static/8211962d4c48829f81a015d71a35024e/111a0/test_result.jpg 2400w\", \"/static/8211962d4c48829f81a015d71a35024e/5c188/test_result.jpg 2560w\"],\n    \"sizes\": \"(max-width: 1200px) 100vw, 1200px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Example Result Plot\"), \"\\n  \")), mdx(\"h1\", {\n    \"id\": \"additional-resources\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Additional Resources\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#additional-resources\",\n    \"aria-label\": \"additional resources permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"As with anything, I learned this method through applying it but also external resources. I wanted to highlight a few that have helped me and might help you!\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://youtu.be/IHZwWFHWa-w\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"3Blue1Brown\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Math educator who essentially started a new genre of how math is taught, amazing!\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://youtu.be/4F0_V_0OO2Q\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"Cyril Stachniss\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Generally a great producer of robotics lectures.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://mml-book.github.io/\",\n    \"target\": \"_self\",\n    \"rel\": \"nofollow\"\n  }, \"Math for Machine Learning\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Freely available textbooks that is a great reference for purposeful mathematics!\")))), mdx(\"h1\", {\n    \"id\": \"closing\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, \"Closing\", mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#closing\",\n    \"aria-label\": \"closing permalink\",\n    \"className\": \"anchor after\"\n  }, mdx(\"svg\", {\n    parentName: \"a\",\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }, mdx(\"path\", {\n    parentName: \"svg\",\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), mdx(\"p\", null, \"I hope you enjoyed this primer to the series, much of the material for the remaining posts is still up in the air so if there are aspects you liked or maybe didn't like let me know!\"), mdx(\"p\", null, mdx(\"figure\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-figure\",\n    \"style\": {}\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"1012px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/57269eca5cbe3ec9bc08b8212a8e2788/427c0/grad_meme.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"95%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAABYlAAAWJQFJUiTwAAAFN0lEQVQ4yy3KbVDTBQDH8T8PIqOZIpMNBriNsbENGRvoGONJBhsMxzOCOBQUfOBhpoh5KoaWoChqJNiJj9eZZ5LWmZRiKZphkFTSg5Z3XVlX0Is6vV5lfbvjevG57/3ufkLj+TEaz4/gPT/GxovjNLRvpM5bRuPW1fR9cIMTd77k9O1x+m9+Ru/QXS6MTjD0zSMujn3FiZv3ODU8zsDo1wzc+5azo18jeC+M0nrxU168NMqGCyOkrihF50hgUcliWs9do+3yZ7Rd+piX3rnN7neG2Xv5NvsH77DvvRE6L4/w6tUxjnw4Tt9HE9OEvuEJBu495MwnE2x9dxT1qrX4O50EuQuRVywl6n/Kqkp0NR4MtR7UngoMq2o59fHnXJl4RN/wfV69/iW9NyYQjl+7xbGrw5y8fouOSx8Rv2YtptUrMdfVYqqpJraiFJE9DbEjHXNNJS7vOrTLigh1Ozj5/hADd+5y7NotXr9+h7fvjiNsf/04rb39tPWfob3/FIWbN+La5MW+oYGCTRtY0tSMrrIMTWUJixvrSa5bQfr6VcRWldPY3Ut7/2l2Hz9B95vn6Dl3DqF2zyFWd/ZQu+cga/b1oPMsR1JWzLzyYgw1K8hZu5ZAu40ZznQCcjOn+TnTkRblT/83Huqj5VAvrYeP8sKBHoT8+vUUNjTjqqujqMGLuqSQ2fkOxK5sZAUuzJVlyFx2ZG4HMrcTaUEOkiXZqEvdVHibKW324m70UtKyhdy6egSdJQH9wvhp2sR49BYTeks8GvOC6R1jNqJNNKJLMhGbmIAmKZ7YJCO6RWYMlgQWWE3EpS5Ek2ZFl2FFUBrjUMbpUcXpiY4zEB2nI9qgm65coyY8Rk1YjBqZWoUsWoksRoVUpSJUoUCqVCBTKQiLVhKuUiJXRyP8MDXJT1OT/DY5yeTUFL9OTfL7n38wePMGSc4cdKkp6DMtyOKjkZtUhBmjmKUIx08kxsfHH19fP3x8fPAPFCGJUCD8BTz5+x+ePoMn/8Kfz57xFOg61k+E2URUvIE5mgj85bMJipyNf1gQvpIAhNki/EUz8Zs5A58ZvvgHiZgTFo4w9uhXRh48ZuThz3z6/S/cffiY4fsPya3yoEg0E2+zojIuIEQVzjzlPIIVEsK1UtSJUYTGBjPfMJfQ2OcQR84kWBaKMDj2HUNf/MjQ/R+5Mv6AWw8ec+jNAZJz87C788lw5ZKZk44j24YpZSFpjlSyXKnYC1MINwajNEmQL5jF3OggAsQihJrWbTS93MHm/d3Ute2krfcoVS1bUFvSkGr1WPNdJNgdhEREEiiTINWGEmOJJM6mxpazEHVcDAp9BNKYEMQyMUJanpOsQjflK1fStHU7a7ZuY5l3M1Ut7dRu20updxeRtiKEuVoC5mkQh2iZFSLHlmUnd0kBxiQLizIzSM7JxJaXhVDv9dKyYyf7e/rYc7CH7Z1deNs6KGnaQWZ1M0ExVgRRBKLnFQi+c/GbIUE+X4vVbmfR4gwkUXLEUglhejWaRCNCw6ZNHDjyGi1tO9nesY+uI0dp7+qmpnkL2VV1JDnd6M3JmCxWEizJpNqzqW9uoqByKWmObNJyHcw3aAiUBeEjDkB4672z7D3cQWd3JyffOMaurr1Ur1tPWXU1JR4P5SuqqfB4yC8rxV7oJsftpqC0nMaWFlzFbiyZadhynZhSkgiNkiNcvf4Wly6fZHDwLO2v7KCwcjl5xeUUlJdRVFnGkvJS8oqLSHc6sDmysdqzSVmchT0/n3UvbMBZXIw51UaqI4OUzAz+AwlAG5ZOIdPVAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Often a Foregone Conclusion...\",\n    \"title\": \"Often a Foregone Conclusion...\",\n    \"src\": \"/static/57269eca5cbe3ec9bc08b8212a8e2788/427c0/grad_meme.png\",\n    \"srcSet\": [\"/static/57269eca5cbe3ec9bc08b8212a8e2788/eed55/grad_meme.png 300w\", \"/static/57269eca5cbe3ec9bc08b8212a8e2788/7491f/grad_meme.png 600w\", \"/static/57269eca5cbe3ec9bc08b8212a8e2788/427c0/grad_meme.png 1012w\"],\n    \"sizes\": \"(max-width: 1012px) 100vw, 1012px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \"), \"\\n    \", mdx(\"figcaption\", {\n    parentName: \"figure\",\n    \"className\": \"gatsby-resp-image-figcaption\"\n  }, \"Often a Foregone Conclusion...\"), \"\\n  \")));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"title":"Neural Networks From Scratch: Gradient Descent","date":"March 25th, 2023","excerpt":"The first in a series where I introduce how to implement gradient descent in the context of Neural Networks.","featureImage":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAEDBQT/xAAWAQEBAQAAAAAAAAAAAAAAAAAAAQL/2gAMAwEAAhADEAAAAd+tx6mgM0CQf//EABoQAAICAwAAAAAAAAAAAAAAAAECAxEAECD/2gAIAQEAAQUCxWvTixFE4bj/xAAUEQEAAAAAAAAAAAAAAAAAAAAg/9oACAEDAQE/AR//xAAWEQEBAQAAAAAAAAAAAAAAAAABESD/2gAIAQIBAT8BAmP/xAAbEAAABwEAAAAAAAAAAAAAAAAAAQIQESAhMf/aAAgBAQAGPwJ86JWdf//EABsQAQACAgMAAAAAAAAAAAAAAAEAEBExIUFR/9oACAEBAAE/IVDbMxFN8U76dGMgY8IFYr//2gAMAwEAAgADAAAAEBcIAP/EABYRAQEBAAAAAAAAAAAAAAAAAAERIP/aAAgBAwEBPxBUYGP/xAAVEQEBAAAAAAAAAAAAAAAAAAABIP/aAAgBAgEBPxBSVj//xAAdEAEBAQACAgMAAAAAAAAAAAABEQAxURAhQWGx/9oACAEBAAE/EKsD51CT7Do6wjwmFIhE/N0ghLdA4n1kuAeP/9k="},"images":{"fallback":{"src":"/static/92b65accb781878b4e5d0395c3c94291/baaed/feature.jpg","srcSet":"/static/92b65accb781878b4e5d0395c3c94291/dd515/feature.jpg 200w,\n/static/92b65accb781878b4e5d0395c3c94291/47930/feature.jpg 400w,\n/static/92b65accb781878b4e5d0395c3c94291/baaed/feature.jpg 800w,\n/static/92b65accb781878b4e5d0395c3c94291/b26d3/feature.jpg 1600w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[]},"width":800,"height":800}}}},"fields":{"path":"/blog/neural-net-gradient-descent/","readingTime":{"text":"27 min read"}},"tableOfContents":{"items":[{"url":"#introduction","title":"Introduction"},{"url":"#prior-knowledge","title":"Prior Knowledge"},{"url":"#gradients","title":"Gradients","items":[{"url":"#gradient-intuition","title":"Gradient Intuition"},{"url":"#gradient-definition","title":"Gradient Definition"}]},{"url":"#gradient-descent","title":"Gradient Descent","items":[{"url":"#descent-intuition","title":"Descent Intuition"},{"url":"#descent-definition","title":"Descent Definition"},{"url":"#local-vs-global-minima","title":"Local vs Global Minima"}]},{"url":"#hill-descent","title":"Hill Descent","items":[{"url":"#surface-and-gradient-equation","title":"Surface and Gradient Equation"},{"url":"#visualizing-the-descent","title":"Visualizing the Descent"}]},{"url":"#linear-regression","title":"Linear Regression","items":[{"url":"#problem-statement","title":"Problem Statement"},{"url":"#function-to-minimize","title":"Function to Minimize"},{"url":"#linear-dataset-example","title":"Linear Dataset Example"},{"url":"#linear-dataset","title":"Linear Dataset"},{"url":"#training-visualization","title":"Training Visualization"},{"url":"#parabolic-dataset-example","title":"Parabolic Dataset Example"},{"url":"#parabolic-dataset","title":"Parabolic Dataset"},{"url":"#training-visualization-1","title":"Training Visualization"},{"url":"#comparing-losses","title":"Comparing Losses"}]},{"url":"#learning-rate-tuning","title":"Learning Rate Tuning","items":[{"url":"#general-behaviors","title":"General Behaviors"},{"url":"#comparing-learning-rates","title":"Comparing Learning Rates"},{"url":"#where-to-start","title":"Where to Start"}]},{"url":"#extending-to-more-variables","title":"Extending to More Variables"},{"url":"#implementation","title":"Implementation","items":[{"url":"#necessary-imports","title":"Necessary Imports"},{"url":"#creating-data","title":"Creating Data"},{"url":"#loss-and-gradient","title":"Loss and Gradient"},{"url":"#example-gradient-descent","title":"Example Gradient Descent"},{"url":"#plotting-results","title":"Plotting Results"}]},{"url":"#additional-resources","title":"Additional Resources"},{"url":"#closing","title":"Closing"}]}},"site":{"siteMetadata":{"title":"Patrick Youssef"}}},"pageContext":{"post_id":"/blog/neural-net-gradient-descent/"}},"staticQueryHashes":[]}